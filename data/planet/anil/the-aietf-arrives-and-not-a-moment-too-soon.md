---
title: The AIETF arrives, and not a moment too soon
description: IETF's new AI Preferences Working Group aims to standardize protocols
  for expressing preferences on AI content collection and processing.
url: https://anil.recoil.org/notes/ai-ietf-aiprefs
date: 2025-02-28T00:00:00-00:00
preview_image:
authors:
- Anil Madhavapeddy
source:
---

<p>The <a href="https://ietf.org">IETF</a> <a href="https://bsky.app/profile/ietf.org/post/3lj6w5fpjx22u">announced</a> their new <a href="https://www.ietf.org/blog/aipref-wg/">AI Preferences Working Group</a> (AIPREF), which will <em>"work on standardizing building blocks that allow for the expression of preferences about how content is collected and processed for Artificial Intelligence models"</em>. This is quite well timed; the IETF tries not to standardise too early before there is <a href="https://www.ietf.org/runningcode/">running code</a> but also needs to move before it's too late and a bad defacto standard is <a href="https://datatracker.ietf.org/doc/html/rfc7282">chosen</a>.  The AI world seems to be at that nexus point right about now, with <a href="https://openai.com/index/introducing-gpt-4-5/">GPT 4.5</a> seemingly hitting a <a href="https://www.newscientist.com/article/2470327-is-openai-hitting-a-wall-with-huge-and-expensive-gpt-4-5-model/">scaling wall</a> and possibly triggering the start of a renewed data scraping frenzy.</p>
<h2>How do websites interact with AI crawlers right now?</h2>
<p>I've found when developing my own website there are a number of approaches to interacting with automated data crawlers. For the record, over 90% of the traffic to this site is from automated sources, so it's a material concern for <a href="https://anil.recoil.org/news?t=selfhosting">selfhosting</a> infrastructure.</p>
<ol>
<li><strong>Ban all bots; humans only plz:</strong> I don't want to do this, as I'd like to opt into my writing training next generation foundation models, but would like some agency over how much I need to pay for them to get their data (I am covering the bandwidth costs here, after all), so I just need them to cooperate more to avoid flooding my site. If I do want to ban them, the excellent <a href="https://github.com/ai-robots-txt/ai.robots.txt/blob/main/table-of-bot-metrics.md">ai-robots</a> crew maintain a useful list of bad bots.</li>
<li><strong>Ban some bots with a robots.txt:</strong> <a href="https://www.rfc-editor.org/rfc/rfc9309.html">RFC9309</a> allows for the discrimination of web-crawlers via a <a href="https://anil.recoil.org/robots.txt">robots.txt</a>. We nowadays have not just a few big crawlers mirroring the Internet (like <a href="https://developers.google.com/search/docs/crawling-indexing/googlebot">Googlebot</a> and <a href="https://en.wikipedia.org/wiki/Bingbot">Bingbot</a>), but seemingly thousands of variants competing for the data gold rush (or in my case, for <a href="https://anil.recoil.org/projects/ce">conservation research</a>!)  The <code>robots.txt</code> doesn't give us enough control to usefully rate-limit across all of these, unfortunately. You need to regenerate the file every time there are new URLs on the site that don't fit a longest-prefix match. This, combined with having a mega <a href="https://sitemaps.org">sitemaps</a> file, is a lot of non-cacheable metadata that's just adding to my serving load.</li>
<li><strong>Add server-side throttling for specific bots:</strong> On the assumption that there are a bunch of bad bots that mimic good bots, what I really need is to start rate-throttling them all! This is where I am today, and ended up hacking together a bunch of OCaml code for <a href="https://anil.recoil.org/notes/bushel-lives">this</a> website to track all the robots request rates and slow down over-eager ones. The rest of the Internet are mostly just asking Cloudflare to take care of this for them, which results in a <a href="https://anil.recoil.org/notes/uk-national-data-lib">world of pain</a> for anyone outside of their world view.</li>
<li><strong>Just give the bots what they want, which is Markdown:</strong> Since I can't really win the throttling wars in the long term, can I just give the bots what they want, which is the core text without all the HTML around it? The first thing these crawlers do is to tokenize the HTML anyway! There is <a href="https://llmstxt.org/">llms.txt</a> emerging for this. I author my website in Markdown in the first place, and then transform it into the HTML you see here. But it looks like the <a href="https://llmstxt.org/domains.html">llms.txt guidelines</a> insist on just one page at the root of the site, and not one Markdown per page. This is probably better for reducing crawling traffic, but it would be a large page even for my humble homepage.</li>
<li><strong>Can I just give you a tarball with my stuff so you leave me alone?:</strong> I rebuild my site regularly, so I could just provide the AI bots with a convenient tar/zip of my entire website content, but put it in a common place so I don't have to pay for the download bandwidth. This could include my images, videos, and source markdown which could be used not only for training, but for <a href="https://archive.org/">archival</a> as well. We don't seem to have a common protocol to map URLs to static archives right now, although there are a <a href="https://en.wikipedia.org/wiki/Web_archive_file">few web archive</a> formats flying around.</li>
</ol>
<h2>The role of the IETF is to create protocols, not mandate implementations</h2>
<p>The IETF has a valuable role to play here to establish a consensus around what a sensible, usable <em>protocol</em> for exchanging data on our websites might look like, rather than mandating any specific backend technology or storage format.
There is a lot of nuance around sharing content over HTTP: it supports <a href="https://http.dev/authentication">authentication</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control">caching</a>, <a href="http://web.archive.org/web/20190904190534/https://www.dirv.me/blog/2011/07/18/understanding-403-forbidden/index.html">access control</a>, <a href="https://matt-jackson.com/seo-glossary/http-429/">rate limiting</a>, and many other features hidden behind a seemingly simple <a href="https://datatracker.ietf.org/doc/html/rfc2616">request-response specification</a>.</p>
<p>I'm hoping that the AIPREF process will end up with something that gives me something closer to 5) above than 1). I need an HTTP-based mechanism by which I can express my preferences for AI crawling, and cooperate with the crawlers so that I can ensure maximum collective benefit to both people and bots visiting my site, rather than withdrawing behind a gated community of humans only. However, I think that this requires the establishment of a protocol to help sequence the HTTP requests together and not just a single static file like <code>llms.txt</code> or <code>sitemap.xml</code>.</p>
<p>Back in the 90s, I <a href="https://anil.recoil.org/papers/netapp-tr-3071">worked</a> <a href="https://anil.recoil.org/papers/netapp-tr-3152">on</a> NetApp/<a href="https://en.wikipedia.org/wiki/NetCache">NetCache</a> with <a href="https://www.netskope.com/press-releases/netskope-john-martin-chief-product-officer">John Martin</a>. Bandwidth used to be expensive and so we deployed edge caches that could <em>modify</em> website content with local modifications to common global content. Consider, for example, a local news website that might want to show mostly cached global news, but also modify the HTML to include local news content. You can do that today via JavaScript, but back then the only way to have a protocol to modify the static HTML. The <a href="https://datatracker.ietf.org/doc/rfc3507/">Internet Content Adaptation Protocol</a> was the IETF's answer to creating a structured HTTP-like protocol to allow edge modifications from proxy servers:</p>
<blockquote>
<p>ICAP is, in essence, a lightweight protocol for executing a "remote procedure call" on HTTP messages.  It allows ICAP clients to pass HTTP messages to ICAP servers for some sort of transformation or other processing ("adaptation").  The server executes its transformation service on messages and sends back responses to the client, usually with modified messages.  Typically, the adapted messages are either HTTP requests or HTTP responses.
<cite>-- <a href="https://datatracker.ietf.org/doc/rfc3507/">RFC3507</a>, IETF</cite></p>
</blockquote>
<p>One of the coolest features of ICAP is that is didn't mandate the transformation mechanism, just the protocol. The proxies deployed at the edge networks would get a vector into transforming the data stream. NetCache implemented an implementation of ICAP, and Squid <a href="https://www.egirna.com/blog/news-2/configure-squid-v6-2-on-ubuntu-server-22-and-use-it-with-icap-18">still supports</a> it. What would a similar approach look like for allowing crawlers into your site's content, but leaving lots of freedom for the details of this to be delegated to the crawlers and servers?</p>
<h2>Challenges in an open data hoovering protocol</h2>
<p><a href="https://bsky.app/profile/aftnet.bsky.social">Antoine Fressancourt</a> <a href="https://bsky.app/profile/aftnet.bsky.social/post/3ljcw2uawe22c">identifies</a> the main problem facing AIPref:</p>
<blockquote>
<p>Given the reports that some current LLM models have been trained on data corpus obtained illegally, I have some doubts that AIPref will be respected.</p>
</blockquote>
<p>This is <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-staff-torrented-nearly-82tb-of-pirated-books-for-ai-training-court-records-reveal-copyright-violations">true</a> for the current generation of data crawlers, but also where the <em>opportunity</em> lies for AIPref. Without a systematic way to support <a href="https://anil.recoil.org/notes/uk-national-data-lib">replication of non-public data</a>, the situation will get even worse as custom apertures are created into data silos without any integrity underlying them.</p>
<p>The main reason for having a protocol-based solution is that we could support the strong authentication and identification of bots. If (for example) the GoogleBot supplied a token with every HTTP request to fetch my content, I could track its use and perhaps even get compensation for the bandwidth costs. The current methods of <a href="https://developers.google.com/search/docs/crawling-indexing/verifying-googlebot">bot verification</a> all seem quite weak; they are just IP based checks for example.</p>
<p>This would in turn open a path to the disciplined negotiation for access controlled data bilaterally between crawlers and hosters. More and more content publishers are signing various <a href="https://www.monda.ai/blog/ultimate-list-of-data-licensing-deals-for-ai">exclusive deals</a> with AI training companies. Irrespective of your opinion on such deals, a protocol to make it easier to authenticate bots strongly would make the establishment (and ongoing negotiation) of those mechanisms far easier to handle.</p>
<p>We are also seeing rapid adoption of the the <a href="https://github.com/modelcontextprotocol">Model Context Protocol</a> released a few months ago. This establishes a <a href="https://github.com/modelcontextprotocol/specification">JSON-RPC specification</a> for LLM clients and data providers to talk to each other locally. It seems odd to me that we'd have a rich "local" specification for data exchange like this for RAG-like systems, but not have one in the wide area across the Internet.  As the chair of the AIPREFS group <a href="https://mnot.net" class="contact">Mark Nottingham</a> notes, <a href="https://www.mnot.net/blog/2024/11/29/platforms">platform advantages</a> are not just network effects, so there may be deep repurcussions into the economics of AI here:</p>
<blockquote>
<p>In short: there are less-recognised structural forces that push key Internet services into centralized, real-time advertising-supported platforms. Along with factors like network effects and access to data, they explain some of why the Internet landscape looks like it does.
<cite>-- <a href="https://www.mnot.net/blog/2024/11/29/platforms">Mark Nottingham</a></cite></p>
</blockquote>
<p>Just substitute "advertising-supported" with "AI" above and the trend becomes clear. The protocol designs we chose today will form structural forces that decide the future of what the post-advertising driven Internet culture and content architecture looks like. It would be a nice outcome to establish open protocols that are somewhere in between the <a href="https://github.com/punkpeye/awesome-mcp-clients">MCP clients</a> and <a href="https://en.wikipedia.org/wiki/Web_server">HTTP servers</a> to facilitate a more equitable outcome rather than pooling all the data to a few big players.</p>
<p>The other consideration is here is that such an open protocol could have utility far beyond "just" managing AI training bots and address the general problem we have that <a href="https://anil.recoil.org/notes/uk-national-data-lib">replicating datasets with access control is difficult</a>. This would help the good folk at <a href="https://archive.org/">Archive.org</a> to manage <a href="https://help.archive.org/help/how-to-download-files/">restricted access</a> data sets that might want to become eventually open. There are also geospatial datasets such as <a href="https://www.gbif.org/">biodiversity data</a> that need help managing how they are mirrored, but with access restrictions for <a href="https://india.mongabay.com/2025/02/commentary-how-data-deficiency-is-hindering-hydro-diplomacy-between-china-and-india/">geopolitical reasons</a>.</p>
<p>Luckily, the IETF do a lot of things over email, so I've signed up to the <a href="https://mailman3.ietf.org/mailman3/lists/ai-control.ietf.org/">AIPREF mailing list</a> to learn more as it develops and hopefully participate!</p>
<small class="credits">
<p>Changelog. Mar 1st 2024: Thanks to <a href="https://mynameismwd.org" class="contact">Michael Dales</a> for spotting typos, and <a href="https://bsky.app/profile/aftnet.bsky.social">Antoine Fressancourt</a> for helpful clarifying questions on Bluesky.</p>
</small>

