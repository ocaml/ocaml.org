<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>http://blog.camlcity.org/blog/rss</id><title type="text">camlcity</title><updated>2023-06-30T05:47:26-00:00</updated><entry><link href="http://blog.camlcity.org/blog/wasicaml1.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;The portability story behind WasiCaml&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
  For a recent project we wrote a compiler that translates a
  domain-specific language (DSL) to some runnable form, and we did
  that in OCaml. The DSL is now part of an Electron-based integrated
  development environment (IDE) that will soon be available
  from &lt;a href=&quot;https://remixlabs.com&quot;&gt;Remix Labs&lt;/a&gt;. Electron runs
  on a couple of operating systems, but the DSL compiler orginally did
  not.  How do we accomplish it to run the DSL compiler on as many
  different operating systems?  This was the question we faced when
  starting the development of WasiCaml, a translator from OCaml
  bytecode to WebAssembly.

&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;
  Of course, Electron is just an example of a cross-platform
  environment.  You can develop apps for Mac, Windows, and Linux, and
  it is Javascript-based.  We picked Electron for porting the user
  interface of the IDE to the desktop - originally the IDE was written
  for the web, and the DSL compiler was running on a server backing
  the web app. Initially, the Electron version of the IDE started just
  a native binary of the DSL compiler as a server process that ran in
  the background, just like we did it for the web, but this means that
  you run into the cross-build problem again that you actually want to
  avoid by running something in Electron: we would have needed to set
  up several build pipelines, one for each OS, in order to build the
  DSL compiler for the targets we wanted to support.

&lt;/p&gt;&lt;p&gt;
  There are already tools to translate OCaml to Javascript (namely
  Bucklescript and js_of_ocaml), and we could have used these to fiddle
  the DSL compiler into the Javascript code base. However, this does
  not feel right: we would have had to reorganize the OCaml code base
  because you can't link in C libraries, and driving the DSL compiler
  would have been quite adventurous (it talks via a bidirectional
  pipeline to its clients). At that time we were already exploring
  WebAssembly for other parts of the system, and the idea came up
  to also use WebAssembly for running the DSL compiler. The
  &lt;a href=&quot;https://github.com/remixlabs/wasicaml/&quot;&gt;WasiCaml&lt;/a&gt;
  project was born (and the translation to Javascript only plan B
  should this turn out to be more difficult than expected).

  &lt;/p&gt;&lt;h2&gt;A quick intro to WebAssembly&lt;/h2&gt;

&lt;p&gt;
  As the name suggests, WebAssembly provides a fairly low-level virtual
  machine for running the code. The instructions are comparable to the
  ones you find in a CPU, e.g. load, store, arithmetic. The code is
  structured into functions which take a fixed number of parameters
  and return a single result. The functions can have local variables
  that can be read and written by the code. The parameters and variables
  can have one of four numeric types (i32, i64, f32, and f64).

&lt;/p&gt;&lt;p&gt;
  For example, this is a WebAssembly module with just one function that
  increments a 32 bit number at a memory location by one, and returns
  the value:
&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
(module
  (import &amp;quot;env&amp;quot; &amp;quot;memory&amp;quot; (memory $memory 1))
  (func $incr (export &amp;quot;incr&amp;quot;) (param $x i32) (result i32)
    (local.get $x)
    (i32.load)
    (i32.const 1)
    (i32.add)
    (return)
  )
)
&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;
  Here, the code is given in the textual format known as WAT. For running
  it, you first need to convert it to the binary format (WASM), e.g.
  with a tool like &lt;a href=&quot;https://github.com/WebAssembly/wabt&quot;&gt;wat2wasm&lt;/a&gt;.

&lt;/p&gt;&lt;p&gt;
  Also note that there is an operands stack: &lt;code&gt;local.get&lt;/code&gt; pushes
  the result on this stack, and &lt;code&gt;i32.load&lt;/code&gt; loads the number
  from the address found on the stack, and also pushes the result on the
  stack. This stack is mainly meant to express the code in a very compact
  way. The engine running code normally translates the stack operations
  into a more efficient form before starting up.

&lt;/p&gt;&lt;p&gt;
  A WebAssembly VM is equipped with linear memory, i.e. the memory
  addresses go from 0 to a maximum address, without fragmentation, and
  without address ranges supporting special semantics like mapped files. The
  memory is only used for data - the running code is inaccessible
  (i.e. the VM has a Harvard architecture), and this also includes the
  call stack and other parts of the VM (e.g. you cannot iterate over
  the local variables of the functions). In order to also support
  indirect jumps, there is a way to reference functions by numeric
  IDs.

&lt;/p&gt;&lt;p&gt;
  Typically, WebAssembly VMs translate the code to the native
  instruction set of the host running of code before running the code
  (often as JIT compilers, but there are now also engines doing the
  translation statically ahead of time, and producing native binaries),
  and these engines almost reach native speed.  All current browsers support
  WebAssembly now, and it is also present in other Javascript-based
  environments (like node, or the Electron platform).  Although it
  started as a web technology, WebAssembly is not limited to the web.
  For example, &lt;a href=&quot;https://docs.wasmtime.dev/&quot;&gt;wasmtime&lt;/a&gt;
  allows you to embed a WebAssembly engine into almost any environment
  - e.g. you could embed the engine into an application server written
  in Go. In this case, there is no Javascript involved at all.
    
  &lt;/p&gt;&lt;h2&gt;WASI&lt;/h2&gt;

&lt;p&gt;
  While the WebAssembly standard defines how to express the code and
  how to run it, there is still the question how to use it with
  popular languages like C, and
  Rust. The &lt;a href=&quot;https://wasi.dev/&quot;&gt;WASI&lt;/a&gt; standard is an ABI
  that answers a lot of the questions. As an ABI it defines calling
  conventions, but it is not limited to that. In particular, there is
  a version of libc that defines a Unix-like set of base functionality
  the language-specific runtime can use. Also, WASI defines a set of host
  functions that play a role comparable to system calls in the WebAssembly
  world, and that allow access to files, the process environment, and
  the current time. With the help of WASI you can compile many C
  or Rust libraries to WebAssembly, and the porting effort is low.

&lt;/p&gt;&lt;p&gt;
  WASI is multi-lingual environment, and you can in particular link
  code written in different languages into the same executable. This is
  possible because the language-specific runtimes have a common foundation
  (libc), and e.g. memory allocated from one language also counts as
  &amp;quot;taken&amp;quot; within the other language.

&lt;/p&gt;&lt;p&gt;
  WASI is still in an early stage. While developing with it I discovered
  a couple of bugs, but the functionality is already impressive and
  usable for many purposes.
  
  &lt;/p&gt;&lt;h2&gt;WasiCaml&lt;/h2&gt;

&lt;p&gt;
  So now, what is WasiCaml, and how can I use it?

&lt;/p&gt;&lt;p&gt;
  Let's assume you have a bytecode executable created by something like
&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  ocamlc -o myexecutable mycode.ml
&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, you can further translate the bytecode executable to WebAssembly:

&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  wasicaml -o mywasm.wasm myexecutable
&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you want to run this executable, you need a specially configured
WebAssembly engine which can be found in ~/.wasicaml/js after installation:

&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  node ~/.wasicaml/js/main.js ./mywasm.wasm ./mywasm.wasm arg ...
&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;code&gt;mywasm.wasm&lt;/code&gt; binary is portable and can be run
  everywhere!

&lt;/p&gt;&lt;p&gt;For simplicity, wasicaml can also generate a wrapper that hides the
&lt;code&gt;node&lt;/code&gt; invocation, and this is triggered by just omitting
the .wasm suffix:

&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  wasicaml -o mywasm myexecutable
&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now you can run the program simply with &lt;code&gt;./mywasm&lt;/code&gt; (but note
that the wrapper is not portable).

&lt;/p&gt;&lt;p&gt;Another option is to link in C libraries like e.g.
&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  wasicaml -o mywasm.wasm myexecutable -cclib ~/.wasicaml/lib/ocaml/libunix.a
&lt;/code&gt;
&lt;/blockquote&gt;

Of course, the C library must also be WASI-compatible.

&lt;p&gt;Note that WasiCaml-produced code can so far not be run with
  wasmtime or wasmer, in particular because there is no machinery
  for exception handling in these engines. Browsers are fully
  supported, though.

  &lt;/p&gt;&lt;h2&gt;The WasiCaml project&lt;/h2&gt;

&lt;p&gt;WebAssembly is still a very new technology and information about it
  is rare. For example, it took a while until I understood that LLVM
  includes a full-featured assembler for WebAssembly, i.e. you can feed
  it a &lt;code&gt;code.s&lt;/code&gt; file, and you get a &lt;code&gt;code.o&lt;/code&gt;
  file back with partially linked WebAssembly code. This is documented
  nowhere, and I could only figure out some parts of the assembler syntax
  by reading the source code of LLVM.

&lt;/p&gt;&lt;p&gt;What I already knew from an earlier WebAssembly project is that
  there is no exception handling (EH) mechanism yet in the standard
  (although this will likely change soon). This turned out as a
  special problem for WasiCaml, because the OCaml runtime uses long
  jumps in external C code to trigger OCaml exceptions. I remembered
  the way the &lt;a href=&quot;https://emscripten.org&quot;&gt;Emscripten&lt;/a&gt;
  toolchain (which is another wrapper around LLVM) gets around this
  difficulty.  If the host language is Javascript, embedded
  WebAssembly code is compiled to run in the same VM that is also used
  to execute Javascript itself, and this means that Javascript exceptions
  also work perfectly for WebAssembly! Of course, this trick is
  really limited to Javascript hosts, but at least I could remove
  the blocker for one of the possible execution environments.

&lt;/p&gt;&lt;p&gt;The very first task was then to get the OCaml bytecode interpreter
  working in a WASI (plus EH) environment.
  
  &lt;/p&gt;&lt;h2&gt;Milestone: running the bytecode interpreter in the WASI environment&lt;/h2&gt;

&lt;p&gt;Essentially, this means that I wanted to (1) clone the OCaml source
  code, (2) &lt;code&gt;configure&lt;/code&gt; it, and (3) &lt;code&gt;make&lt;/code&gt; the
  bytecode interpreter (and the whole OCaml bytecode toolchain).  The
  C compiler comes from the
  &lt;a href=&quot;https://github.com/WebAssembly/wasi-sdk&quot;&gt;WASI SDK&lt;/a&gt;,
  and it compiles directly to WebAssembly. Now, if you just set the
  &lt;code&gt;CC&lt;/code&gt; variable to this C compiler, &lt;code&gt;configure&lt;/code&gt;
  will consider the target as a cross-compile target. Such targets
  are still very tricky, and - because we actually &lt;em&gt;can&lt;/em&gt; run
  the code somehow - I thought it is better to avoid cross-compilation
  altogether, and to add some tooling so that binaries are
  directly runnable.

&lt;/p&gt;&lt;p&gt;Instead of pointing &lt;code&gt;CC&lt;/code&gt; directly to the C compiler of
  the WASI SDK, there is now a wrapper script &lt;code&gt;wasi_cc&lt;/code&gt;.
  The main purpose of this script is to reshape the WebAssembly
  executables so that they are directly runnable on the host
  system. This is accomplished by prepending a &lt;em&gt;starter&lt;/em&gt; to the
  WebAssembly code. The &lt;em&gt;starter&lt;/em&gt; runs &lt;code&gt;node&lt;/code&gt; with
  the right driver script, and extracts the WebAssembly code from the
  executable file. For example, if you do

&lt;/p&gt;&lt;blockquote&gt;
&lt;code style=&quot;white-space: pre&quot;&gt;
  wasi_cc -o ex code.c
&lt;/code&gt;
&lt;/blockquote&gt;

the resulting file &lt;code&gt;ex&lt;/code&gt; can be directly run with
&lt;code&gt;./ex&lt;/code&gt;.

&lt;p&gt;With this trick, &lt;code&gt;configure&lt;/code&gt; now &amp;quot;thinks&amp;quot; that the
  target is a native target of the operating
  system. &lt;code&gt;configure&lt;/code&gt; could also run the tests on the
  existence of the various libc library functions the OCaml runtime
  needs, and figured out a lot of that stuff correctly. Nevertheless,
  not everything was working, and I had to fork the OCaml sources in
  order to disable functions that are not available
  (see &lt;a href=&quot;https://github.com/gerdstolpmann/ocaml/compare/4.12.0...gerd/wasi-4.12.0&quot;&gt;gerd/wasi-4.12.0&lt;/a&gt;
  for the changes).

&lt;/p&gt;&lt;p&gt;In this branch of OCaml I also changed the main function of the
  bytecode interpreter so that it catches exceptions from Javascript
  (actually, this function was split into two, and the outer function
  catches the exception thrown by the inner function).

&lt;/p&gt;&lt;p&gt;A final difficulty was that function pointers in WebAssembly are typed
  - which is a logical consequence of the fact that functions are typed.
  OCaml generates a file &lt;code&gt;prims.c&lt;/code&gt; that initializes the list
  of FFI functions, and initially LLVM did not like this file, because
  it could not infer the types of the function pointers. The solution
  was &lt;em&gt;not&lt;/em&gt; to generate WebAssembly for this single file but
  to leave it as LLVM IR (&amp;quot;bitcode&amp;quot;). In this format function pointers
  can remain untyped, and the LLVM linker is smart enough to fix up
  the problem at link time, and to convert LLVM IR to WebAssembly when
  the types of the FFI functions are known.

&lt;/p&gt;&lt;p&gt;With this trick, everything worked fine! The speed of the bytecode
  interpreter did not slow much down in WebAssembly, which was very
  encouraging.

  &lt;/p&gt;&lt;h2&gt;Milestone: the direct translator&lt;/h2&gt;

&lt;p&gt;After the bytecode interpreter was running, the second step was to
  directly generate WebAssembly code from OCaml. Actually, there were
  two choices: either to pick up one of the internal formats of OCaml
  (e.g. &amp;quot;Lambda&amp;quot; or &amp;quot;C--&amp;quot;) and to change the OCaml compiler directly,
  or to take the bytecode as the starting point. I preferred the
  latter because WasiCaml is then an add-on processor that can be
  easily added to existing OCaml projects, and because some
  difficulties could be avoided (e.g. incremental compilation, and
  many many fixups through the whole toolchain). Also, I hoped that
  the resulting speed would still be &amp;quot;good enough&amp;quot; (at least for the
  purposes of the DSL compiler we wanted to run with WebAssembly).

&lt;/p&gt;&lt;p&gt;Also, bytecode made it also a lot easier for me to get started.
  There were really a lot of unanswered questions: what does the
  function call mechanism look like? How do we get around the problem
  that OCaml code typically requires tail calls to be working but
  there aren't tail calls in WebAssembly (yet)? What does the code
  look to allocate a block of memory? How do we emulate exceptions?
  Picking bytecode meant that I could focus on these questions, while
  the bytecode instructions could initially be translated in a naive
  way, e.g. by translating each bytecode instruction separately to a
  fixed block of WebAssembly instructions (like instantiating a
  template). (Note that the current WasiCaml compiler is already
  a lot better than that.)

&lt;/p&gt;&lt;p&gt;Picking bytecode also meant that WasiCaml inherits the bytecode
  stack. This is actually not a bad thing - because of OCaml's memory
  management the stack must reside in addressable memory, and the
  bytecode stack could serve as what the WebAssembly community calls
  a &lt;em&gt;shadow stack&lt;/em&gt;. (Even for the C language there is a
  shadow stack - and the alternative would have been to also use the
  shadow stack of the C language.) So we got the shadow stack for
  OCaml code practically for free.

&lt;/p&gt;&lt;p&gt;The stack is important because the garbage collector must be
  able to run over all locations where OCaml values are stored.
  As already mentioned, the locations WebAssembly natively supports
  cannot be traversed over (like local and global variables), and
  hence it is crucial to put OCaml values into memory whenever there
  is the chance of a garbage collector run.

&lt;/p&gt;&lt;p&gt;Note that the native OCaml compiler is not much different in this
  respect - only that the native stack of the operating system can be
  used for storing values because it resides in memory. The details
  are different, though. When a value is moved temporarily to the
  stack, this is usually called &amp;quot;register spilling&amp;quot;, and this is done
  because (1) there is only a limited amount of registers, but another
  register is needed, or (2) you don't know which register remains
  untouched when you call a function, or (3) you call some code that
  may run the garbage collector. Now, in WebAssembly, reason (1) is
  never the case because there can be any number of local variables
  (which take over the role of registers), and the details of (3) are
  very different, because in a native environment the registers are
  global stores, permitting some time-saving tricks that are
  unavailable in WebAssembly.

&lt;/p&gt;&lt;p&gt;So, for developing the WasiCaml code emitter, this meant that it
  had to follow constraints so that OCaml values end up on the stack
  in the right moment. Actually, these constraints mainly shaped the
  layout of the WasiCaml code.
  
  &lt;/p&gt;&lt;h2&gt;32 bit comes back!&lt;/h2&gt;

&lt;p&gt;Once WasiCaml was working, we got back to the DSL compiler we
  originally wanted to make cross-platform. And we actually got it
  running! There was one remaining problem, though: WebAseembly is a
  32 bit environment. As you may know, OCaml suffers from some
  limitations in this case. Most annoyingly, strings can only be 16 MB
  in size at most.

&lt;/p&gt;&lt;p&gt;Fortunately, this problem occurred only here and there, mostly
  in the code emitter. Here, we could switch to
  &lt;a href=&quot;https://github.com/Chris00/ocaml-rope&quot;&gt;ropes&lt;/a&gt;
  as alternate representation - and, lucky as we were, it turned
  out that this change did not eat much performance.

&lt;/p&gt;&lt;p&gt;The DSL compiler is quite big, and the WebAssembly version takes
  around 3 seconds to start up. This is longer than usual, but for
  our application we could hide the startup time, and are now quite
  happy with the product.

  &lt;/p&gt;&lt;hr/&gt;

&lt;p&gt;PS. Interested in WebAssembly and you know OCaml (or another
  functional language like Elm, Scala, Haskell, ...)?
  &lt;a href=&quot;https://www.mixtional.de/recruiting/2021-01/index.html&quot;&gt;We might have
    a job for you (July 2021)&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann is the CEO of &lt;a href=&quot;https://mixtional.de&quot;&gt;Mixtional Code GmbH&lt;/a&gt;, currently busy with the last development steps of the &lt;a href=&quot;http://remixlabs.com&quot;&gt;Remix Labs&lt;/a&gt; platform

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/wasicaml1.html</id><title type="text">WasiCaml: Translate OCaml Code to WebAssembly</title><updated>2021-07-15T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/omake3.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;Faster builds with omake, part 3: Caches&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
In this (last) part of the series we have a closer look at how OMake uses
caches, and what could be improved in this field. Remember that we saw
in total double speed for large OMake projects, and that we also could
reduce the time for incremental builds. In particular for the latter, the
effect of caching is important.

&lt;cc-field name=&quot;maintext&quot;&gt;
&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-bottom: 1em; margin-top: 1em; background-color: #E0E0E0&quot;&gt;
This text is part 3/3 of a series about the OMake improvements
sponsored by &lt;a href=&quot;http://lexifi.com&quot;&gt;LexiFi&lt;/a&gt;:
&lt;ul&gt;
  &lt;li&gt;Part 1: &lt;a href=&quot;http://blog.camlcity.org/blog/omake1.html&quot;&gt;Overview&lt;/a&gt;
  &lt;/li&gt;&lt;li&gt;Part 2: &lt;a href=&quot;http://blog.camlcity.org/blog/omake2.html&quot;&gt;Linux&lt;/a&gt;
  &lt;/li&gt;&lt;li&gt;Part 3: Caches (this page)
&lt;/li&gt;&lt;/ul&gt;
The original publishing is on &lt;a href=&quot;http://blog.camlcity.org/blog&quot;&gt;camlcity.org&lt;/a&gt;.
&lt;/div&gt;
&lt;p&gt;
Caching more is better, right? Unfortunately, this attitude of many
application programmers does not hold if you look closer at how caches
work. Basically, you trade memory for time, but there are also unwanted
effects. As we learned in the last part, bigger process images may also
cost time. What we examined there at the example of the fork() system
call is also true for any memory that is managed in a fine-grained
way. Look at the garbage collector of the OCaml runtime: If more memory
blocks are allocated, the collector also needs to cycle through more
blocks in order to mark and reclaim memory. Although the runtime includes
some clever logic to alleviate this effect (namely by allowing more waste
for bigger heaps and by adjusting the collection speed to the allocation
speed), the slowdown is still measurable.

&lt;/p&gt;&lt;p&gt;
Another problem for large setups is that if processes consume more
memory the caches maintained by the OS have less memory to work with.
The main competitor on the OS level is the page cache that stores
recently used file blocks. After all, memory is limited, and it is
the question for what we use it. Often enough, the caches on the OS
level are the most effective ones, and user-maintained caches need
to be justified.

&lt;/p&gt;&lt;p&gt;
In the case of OMake there are mainly two important caches:

&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;The target cache answers the question whether a file can be built in
    a given directory. The cache covers both types of build rules: explicit
    and implicit rules. For the latter it is very important to have this
    cache because the applicable implicit rules need to be searched.
    As OMake normally uses the &amp;quot;-modules&amp;quot; switch of ocamldep, it has to
    find out on its own in which directory an OCaml module is built.
&lt;/li&gt;&lt;li&gt;The file cache answers the question whether a file is still up to date,
    or whether it needs to be rebuilt. This is based on three data blobs:
    first, the Unix.stat() properties of the file (and whether the file
    exists at all). Second, the MD5 digest of the file. Third, the digest
    of the command that created the file. If any of these blobs change
    the file is out of date. The details are somewhat complicated, though,
    in particular the computation of the digest costs some time and should
    only be done if it helps avoiding other expensive actions. Parts of the file
    cache survive OMake invocations as these are stored in the &amp;quot;.omakedb&amp;quot;
    file.
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
All in all, I was looking for ways of reducing the size of the caches, and
for a cleverer organization that makes the cache operations cheaper.

&lt;/p&gt;&lt;h2&gt;The target cache&lt;/h2&gt;

The target cache is used for searching the directory where a file can be
built, and also the applicable file extensions (e.g. if a file m.ml
is generated from m.mly there will be entries for both m.ml and m.mly).
As I found it, it was very simple, just a mapping

&lt;blockquote&gt;
filepath &amp;#8614; buildable_flag
&lt;/blockquote&gt;

and if a file f could potentially exist in many directories d there
was a separate entry d/f for every d. For a given OCaml module m,
there were entries for every potential suffix (i.e. for .cmi, .cmo, .cmx
etc.), and also for the casing of m (remember that a module M can be
stored in both m.ml and M.ml). In total, the cache had 2 * D * S * M
entries (when D = number of build directories and S = number of file
suffixes). It's a high number of entries.

&lt;p&gt;
The problem is not only the size, but also the speed: For every test
we need to walk the mapping data structure.

&lt;/p&gt;&lt;p&gt;
The new layout of the cache compresses the data in the following way:

&lt;/p&gt;&lt;blockquote&gt;
filename &amp;#8614; (directories_buildable, directories_non_buildable)
&lt;/blockquote&gt;

On the left side, only simple filenames without paths are used. So
we need only 1/D entries than before now. On the right side, we have
two sets: the directories where the file can be built, and the directories
where the file cannot be built (and if a directory appears in neither
set, we don't know yet). As the number of directories is very limited,
these sets can be represented as bitsets.

&lt;p&gt;
Note that if we were to program a lame build system, we could even
simplify this to

&lt;/p&gt;&lt;blockquote&gt;
filename &amp;#8614; directory_buildable option
&lt;/blockquote&gt;

but we want to take into account that files can potentially be built in
several directories, and that it depends on the include paths currently
in scope which directory is finally picked.

&lt;p&gt;
It's not only that the same information is now stored in a compressed
way. Also, the main user of the target cache picks a single file and
searches the directory where it can be built. Because the data structure
is now aligned with this style of accessing it, only one walk over the
mapping is needed per file (instead of one walk per combination of directory
and file). Inside the loop over the directories we only need to look into
the bitsets, which is very cheap.



&lt;/p&gt;&lt;h2&gt;The file cache&lt;/h2&gt;

Compared to the target cache, the file cache is really complicated. For
every file we have three meta data blobs (stat, file digest, command
digest). Also, there are two versions of the cache: the persistent
version, as stored in the .omakedb file, and the live version.

&lt;p&gt;
Many simpler build systems (like &amp;quot;make&amp;quot;) only use the file stats for
deciding whether a file is out of date. This is somewhat imprecise,
in particular when the filesystem stores the timestamps of the files
with only low granularity (e.g. in units of seconds). Another problem
occurs when the timestamps are not synchronous with the system clock,
as it happens with remote filesystems.

&lt;/p&gt;&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-top: 1em; background-color: #E0E0E0&quot;&gt;
There is a now a &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork/tags&quot;&gt;pre-release omake-0.10.0-test1&lt;/a&gt; that can be bootstrapped! It contains all
of the described improvements, plus a number of bugfixes.
&lt;/div&gt;

&lt;p&gt;
OMake is programmed so that it only uses the timestamps between
invocations. This means that if OMake is started another time, and the
timestamp of a file changed compared with the previous invocation of
OMake, it is assumed that the file has changed. OMake does not use
timestamps during its runs. Instead it relies on the file cache as the
instance that decides which files need to be created again. For doing
so, it only uses digests (i.e. a rule fires when the digests of the
input files change, or when the digest of the command changes).

&lt;/p&gt;&lt;p&gt;
The role of the .omakedb file is now that a subset of the file cache
is made persistent beween invocations. This file stores the timestamps
of the files and the digests. OMake simply assumes that the saved digest
is still the current one if the timestamp of the file remains the same.
Otherwise it recomputes the digest. This is the only purpose of the
timestamps. Inaccuracies do not play a big role when we can assume that
users typically do not start omake instances so quickly after each other
that clock deviations would matter.

&lt;/p&gt;&lt;p&gt;
The complexity of the file cache is better understood if you look at
key operations:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;Load the .omakedb file and interpret it in the right way
  &lt;/li&gt;&lt;li&gt;Decide whether the cached file digest can be trusted or not
      (and in the latter case the digest is recomputed from the existing
      file)
  &lt;/li&gt;&lt;li&gt;Decide whether a rule is out of date or not. This check needs
      to take the cache contents for the inputs and the outputs of
      the rule into account.
  &lt;/li&gt;&lt;li&gt;Sometimes, we want to avoid expensive checks, and e.g. only know
      whether a digest might be out of date from the available information
      without having to recompute the digest.
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
After finding a couple of imprecise checks in the existing code, I
actually went through the whole Omake_cache module, and went through
all data cases. After that I'm now sure that it is perfect in the sense
that only those digests are recomputed that are really needed for
deciding whether a rule is out of date.

&lt;/p&gt;&lt;p&gt;
There are also some compressions:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;The cache no longer stores the complete Unix.stat records, but only
      the subset of the fields that are really meaningful (timestamps, inode),
      and represent these fields as a single string.
  &lt;/li&gt;&lt;li&gt;There is a separate data structure for the question whether a file
      exists. This is one of the cases where OS level caches already do a
      good job. Now, only for the n most recently accessed files this
      information is available (where n=100). On Linux with its fast system
      calls this cache is probably unnecessary, but on Windows I actually saw some
      speedup.
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
All taken together, this gives another little boost. This is mostly observable
on Windows as this OS does not profit from the improvements described in the
previous article of the series.

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/omake3_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;
&lt;/p&gt;&lt;/cc-field&gt;
&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as OCaml consultant.

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/omake3.html</id><title type="text">OMake On Steroids (Part 3)</title><updated>2015-06-23T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/omake2.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;Faster builds with omake, part 2: Linux&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
The Linux version of OMake suffered from specific problems, and it is
worth looking at these in detail.

&lt;/div&gt;

&lt;div&gt;
  
&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-bottom: 1em; background-color: #E0E0E0&quot;&gt;
This text is part 2/3 of a series about the OMake improvements
sponsored by &lt;a href=&quot;http://lexifi.com&quot;&gt;LexiFi&lt;/a&gt;:
&lt;ul&gt;
  &lt;li&gt;Part 1: &lt;a href=&quot;http://blog.camlcity.org/blog/omake1.html&quot;&gt;Overview&lt;/a&gt;
  &lt;/li&gt;&lt;li&gt;Part 2: Linux (this page)
  &lt;/li&gt;&lt;li&gt;Part 3: Caches (will be released on Tuesday, 6/23)
&lt;/li&gt;&lt;/ul&gt;
The original publishing is on &lt;a href=&quot;http://blog.camlcity.org/blog&quot;&gt;camlcity.org&lt;/a&gt;.
&lt;/div&gt;
&lt;p&gt;While analyzing the performance characteristics of OMake, I found
that the features of the OS were used in a non-optimal way. In
particular, the fork() system call can be very expensive, and by
avoiding it the speed of OMake could be dramatically improved. This is
the biggest contribution to the performance optimizations allowing
OMake to run roughly twice as fast on Linux
(see &lt;a href=&quot;http://blog.camlcity.org/blog/omake1.html&quot;&gt;part 1&lt;/a&gt; for numbers).

&lt;/p&gt;&lt;h2&gt;The fork/exec problem&lt;/h2&gt;
&lt;p&gt;
The traditional way of starting commands is to use the fork/exec
combination: The fork() system call creates an almost identical copy
of the process, and in this copy the exec() call starts the
command. This has a number of logical advantages, namely that you can
run code between fork() and exec() that modifies the environment for
the new command. Often, the file descriptors 0, 1, and 2 are assigned
as it is required for creating pipelines. You can also do other
things, e.g. change the working directory.

&lt;/p&gt;&lt;p&gt;
The whole problem with this is that it is slow. Even for a modern OS
like Linux, fork() includes a number of expensive operations. Although
it can be avoided to actually copy memory, the new address space must
be set up by duplicating the page table. This is the more expensive the
bigger the address space is. Also, memory must be set aside even if it
is not immediately used. The entries for all file mappings must be
duplicated (and every linked-in shared library needs such mappings).
The point is now that all these actions are not really needed because
at exec() time the whole process image is replaced by a different one.

&lt;/p&gt;&lt;p&gt;
In my performance tests I could measure that forking a 450 MB process
image needs around 10 ms. In the n=8 test for compiling each of the
4096 modules two commands are needed (ocamldep.opt and ocamlopt.opt).
The time for this fork alone sums up to 80 seconds. Even worse, this
dramatically limits the benefit of parallelizing the build, because
this time is always spent in the main process.

&lt;/p&gt;&lt;p&gt;
The POSIX standard includes an alternate way of starting commands, the
posix_spawn() call. It was originally developed for small systems
without virtual memory where it is difficult to implement fork()
efficiently. However, because of the mentioned problems of the
fork/exec combinations it was quickly picked up by all current POSIX
systems.  The posix_spawn() call takes a potentially long list of
parameters that describes all the actions needed to be done between
fork() and exec().  This gives the implementer all freedom to exploit
low-level features of the OS for speeding the call up. Some OS, e.g.
Mac OS X, even implement posix_spawn directly as system call.

&lt;/p&gt;&lt;p&gt;
On Linux, posix_spawn is a library function of glibc. By default,
however, it is no real help because it uses fork/exec (being very
conservative).  If you pass the flag POSIX_SPAWN_USEVFORK, though, it
switches to a fast alternate implementation. I was pointed (by T&amp;ouml;r&amp;ouml;k
Edwin) to a few emails showing that the quality in glibc is not yet
optimal. In particular, there are weaknesses in signal handling and in
thread cancellation. Fortunately, these weaknesses do not matter for
this application (signals are not actively used, and on Linux OMake is
single-threaded).

&lt;/p&gt;&lt;p&gt;
Note that I developed the wrapper for posix_spawn already years ago
for OCamlnet where it is still used. So, if you want to test the speed
advantage out on yourself, just use OCamlnet's Shell library for
starting commands.

&lt;/p&gt;&lt;h2&gt;Pipelines and fork()&lt;/h2&gt;

&lt;p&gt;It turned that there is another application of fork() in OMake. When
creating pipelines, it is sometimes required that the OMake process
forks itself, namely when one of commands of the pipeline is
implemented in the OMake language. This is somewhat expected, as the
parts of a pipeline need to run concurrently. However, this feature
turned out to be a little bit in the way because the default build
rules used it. In particular, there is the pipeline

&lt;/p&gt;&lt;blockquote&gt;
&lt;code&gt;&lt;small&gt;
$(OCAMLFIND) $(OCAMLDEP) ... -modules $(src_file) | ocamldep-postproc
&lt;/small&gt;&lt;/code&gt;
&lt;/blockquote&gt;

which is started for scanning OCaml modules. While the first command,
$(OCAMLFIND), is a normal external command, the second command,
ocamldep-postprocess, is written in the OMake language.

&lt;p&gt;Forking for creating pipelines is even more expensive than the
fork/exec combination discussed above, because memory needs really to
be copied. I could finally avoid this fork() by some trickery in the
command starter. When used for scanning, and the command is the last one
in the pipeline (as in the above pipeline), a workaround is activated
that writes the data to a temporary file, as if the pipeline would read

&lt;/p&gt;&lt;blockquote&gt;
&lt;code&gt;&lt;small&gt;
$(OCAMLFIND) $(OCAMLDEP) ... -modules $(src_file) &amp;gt;$(tmpfile);&lt;br/&gt;
ocamldep-postproc &amp;lt;$(tmpfile)
&lt;/small&gt;&lt;/code&gt;
&lt;/blockquote&gt;

&lt;p&gt;(NB. You actually can also program this in the OMake language. However,
this does not solve the problem, because for sequences of commands
$(cmd1);$(cmd2) it is also required to fork the process. Hence, I had to
find a solution deeper in the OMake internals.)

&lt;/p&gt;&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-top: 1em; background-color: #E0E0E0&quot;&gt;
There is a now a &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork/tags&quot;&gt;pre-release omake-0.10.0-test1&lt;/a&gt; that can be bootstrapped! It contains all
of the described improvements, plus a number of bugfixes.
&lt;/div&gt;

&lt;p&gt;There is one drawback of this, though: The latency of the pipeline is
increased when the commands are run sequentially rather than in parallel.
The effect is that OMake takes longer for a j=1 build even if less CPU
resources are consumed. A number of further improvements compensate for
this:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;Most importantly, ocamldep-postprocess can now use a builtin function,
      speeding this part up by switching the implementation language (now
      OCaml, previously the OMake language).
  &lt;/li&gt;&lt;li&gt;Because ocamldep-postprocess mainly accesses the target cache,
      speeding up this cache also helped (see the next part of this
      article series).
  &lt;/li&gt;&lt;li&gt;Finally, there is now a way how functions like ocamldep-postprocess
      can propagate updates of the target cache to the main environment.
      The background is here that functions implementing commands run in
      a sub environment simulating some isolation from the parent
      environment. This isolation prevented that updates of the target
      cache found by one invocation of ocamldep-postprocess could be used
      by the next invocation. This also speeds up this function.
&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;Windows is not affected&lt;/h2&gt;

&lt;p&gt;The Windows port of OMake is not affected by the fork problems. For
starting commands, an optimized technique similar to posix_spawn() is
used anyway. For pipelines and other internal uses of fork() the
Windows port uses threads. (Note beside: You may ask why we don't use
threads on Linux. There are a couple of reasons: First, the emulation
of the process environment with threads is probably not quite as
stable as the original using real processes. Second, there are
difficult interoperability problems between threads and signals
(something that does not exist in Windows).  Finally, this would not
save us maintaining the code branch using real processes and fork()
because OCaml does not support multi-threading for all POSIX systems.
Of course, this does not mean we cannot implement it as optional
feature, and probably this will be done at some point in the future.)

&lt;/p&gt;&lt;p&gt;The trick of using temporary files for speeding up pipelines is not
enabled on Windows. Here, it is more important to get the benefits of
parallelization that the real pipeline allows.

&lt;/p&gt;&lt;div style=&quot;border: 1px solid black; padding: 10px; margin-left: 1em; margin-bottom: 1em; background-color: #E0E0E0&quot;&gt;
The next part will be published on Tuesday, 6/23.
&lt;/div&gt;

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/omake2_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;


&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as OCaml consultant.

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/omake2.html</id><title type="text">OMake On Steroids (Part 2)</title><updated>2015-06-19T12:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/omake1.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;Faster builds with omake, part 1: Overview&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
In
the &lt;a href=&quot;https://sympa.inria.fr/sympa/arc/caml-list/2014-09/msg00090.html&quot;&gt;2014
edition&lt;/a&gt; of the &amp;quot;which is the best build system for OCaml&amp;quot; debate
the &lt;a href=&quot;http://omake.metaprl.org&quot;&gt;OMake&lt;/a&gt; utility was heavily
criticized for being not scalable enough. Some quick tests showed that
there was in deed a problem. At
&lt;a href=&quot;http://lexifi.com&quot;&gt;LexiFi&lt;/a&gt;, the size of the source tree obviously
already exceeded the critical point, and LexiFi was interested in an
improvement. LexiFi develops for both Linux and Windows, and
OMake is their preferred build system because of its excellent support
for Windows. The author of these lines got some funding from LexiFi
for analyzing and fixing the problem.

&lt;/div&gt;

&lt;div&gt;
  
&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-bottom: 1em; background-color: #E0E0E0&quot;&gt;
This text is part 1/3 of a series about the OMake improvements
sponsored by &lt;a href=&quot;http://lexifi.com&quot;&gt;LexiFi&lt;/a&gt;:
&lt;ul&gt;
  &lt;li&gt;Part 1: Overview (this page)
  &lt;/li&gt;&lt;li&gt;Part 2: Linux (will be released on Friday, 6/19)
  &lt;/li&gt;&lt;li&gt;Part 3: Caches (will be released on Tuesday, 6/23)
&lt;/li&gt;&lt;/ul&gt;
The original publishing is on &lt;a href=&quot;http://blog.camlcity.org/blog&quot;&gt;camlcity.org&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;
OMake is not only a build system (like e.g. ocamlbuild), but it also
includes extensions that are important for controlling and customizing
builds. There is an interpreter for a simple dynamically typed
functional language. There is a command shell implementing utilities
like &amp;quot;rm&amp;quot; or &amp;quot;cp&amp;quot; which is in particular important on non-Unix
systems. There are system interfaces for watching files and restarting
the build whenever source code is saved in the editor. In short, OMake
is very feature-rich, but also, and this is the downside, it is also
quite complex: around 130 modules and 80k lines of code. Obviously, it
is easy to overlook performance problems when so much code is
involved. For me as the developer seeing the sources for the first
time the size was also a challenge, namely for identifying possible
problems and for finding solutions.

&lt;/p&gt;&lt;h2&gt;Quantifying the performance problem&lt;/h2&gt;

My very first activity was to develop a synthetic benchmark for OMake
(and actually, for any type of OCaml build system). Compared with a
real build, a synthetic benchmark has the big advantage that you can
simulate builds of any size. The benchmark has these characteristics:
The task is to build n^2 libraries with n^2 modules each (for a given
small number n), and the dependencies between the modules are created
in a way so that we can stress both the dependency analyzer of the
build utility and the ability to run commands in parallel. In
particular, every library would allow n parallel build flows of the
n^2 modules, and you can build n of the n^2 libraries in
parallel. (For details see the &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork/blob/perf-test/performance/generate.ml&quot;&gt;source code&lt;/a&gt;.)

&lt;p&gt;
This is what I got for omake-0.9.8.6 (note that a different computer
was used for Windows, so you cannot compare Linux with Windows):

&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th&gt;Size n&lt;/th&gt;
&lt;th&gt;Parallelism j&lt;/th&gt;
&lt;th&gt;Number of modules (n^4)&lt;/th&gt;
&lt;th&gt;Runtime Linux&lt;/th&gt;
&lt;th&gt;Runtime Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=7&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2401&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;645&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;353&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=7&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=4&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2401&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;213&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=8&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;4096&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;1906&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;877&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=8&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=4&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;4096&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;607&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;341&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This clearly shows that there is something wrong, in particular for
Linux as OS: For the n=8 number of 4096 modules, which is around 1.7
times of the 2401 modules for n=7, omake needs around three times
longer (for a single-threaded build). For Windows, the numbers are
slightly better: the n=8 build takes 2.5 of the time of the n=7
build. Nevertheless, this is quite far away from the optimum.

&lt;/p&gt;&lt;p&gt;Note that this is not good, but it is also not a catastrophe. The
latter shows up if you try to use ocamlbuild. I couldn't manage to
build the n=7 test case at all: after 30 minutes ocamlbuild slowed
down to a crawl, and progressed only with a speed of around one module
per second. Apparently, there are much worse problems than with
OMake. (Btw, it would be nice to hear how other build systems
compete.)

&lt;/p&gt;&lt;h2&gt;After improving OMake&lt;/h2&gt;

The version from today (2015-05-18)
at &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork&quot;&gt;Github&lt;/a&gt;
behaves much better:

&lt;p&gt;
&lt;/p&gt;&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th&gt;Size n&lt;/th&gt;
&lt;th&gt;Parallelism j&lt;/th&gt;
&lt;th&gt;Number of modules (n^4)&lt;/th&gt;
&lt;th&gt;Runtime Linux&lt;br/&gt;(Speedup factor)&lt;/th&gt;
&lt;th&gt;Runtime Windows&lt;br/&gt;(Speedup factor)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=7&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2401&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;169 (3.8)&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;317 (1.1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=7&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=4&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;2401&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;59 (3.6)&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;163 (1.1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=8&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;4096&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;363 (5.3)&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;661 (1.3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=8&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;j=4&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;4096&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;144 (4.2)&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;330 (1.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div style=&quot;float:right; width:50%; border: 1px solid black; padding: 10px; margin-left: 1em; margin-top: 1em; background-color: #E0E0E0&quot;&gt;
There is a now a &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork/tags&quot;&gt;pre-release omake-0.10.0-test1&lt;/a&gt; that can be bootstrapped! It contains all
of the described improvements, plus a number of bugfixes.
&lt;/div&gt;

&lt;p&gt;As you can see, there is a huge improvement for Linux and a slight
one for Windows. It turns out that the Linux version ran into a
Unix-specific issue of starting commands from a big process (the OMake
main process reaches around 450MB). OMake used the conventional
fork/exec combination for doing so, but it is a known problem that
this does not work well for big process images. We'll come to the
details of this later. The Windows version never suffered from this
problem.

&lt;/p&gt;&lt;p&gt;The scalability is now somewhat better, but still not great. For both
Windows and Linux, the n=8 runs take now around 2.1 times longer than the
n=7 runs.

&lt;/p&gt;&lt;p&gt;Another aspect of the performance impression is how long a typical
incremental build takes after changing a single file. At least for
OMake, a good measure for this is the zero rebuild time: how long
OMake takes to figure out that nothing has changed, i.e. the time for
the second omake run in &amp;quot;omake ; omake&amp;quot;:

&lt;/p&gt;&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th&gt;Parameters&lt;/th&gt;
&lt;th&gt;Runtime Linux omake-0.9.8.6&lt;/th&gt;
&lt;th&gt;Runtime Linux 2015-05-18&lt;br/&gt;(Speedup Factor)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=7, j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;16.8&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;8.4 (2.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;right&quot;&gt;n=8, j=1&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;39.2&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;15.6 (2.5)&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The time roughly halves. Note that you get a similar effect under
Windows as OMake doesn't start any commands for a zero
rebuild. Actually, most time is spent for constructing the internal
data structures and for computing digests (not only for files but also
for commands, which turns out to be the more expensive action).


&lt;/p&gt;&lt;h2&gt;How to tackle the analysis&lt;/h2&gt;

I started it the old-fashioned way by manually instrumenting
interesting functions. This means that counts and (wall-clock)
runtimes are measured. Functions that (subjectively) &amp;quot;take too long&amp;quot;
are further analyzed by also instrumenting called functions. This way
I could quickly find out the interesting parts (while learning how
OMake works as you go through the code and instrument it). The
helper module I used: &lt;a href=&quot;https://github.com/gerdstolpmann/omake-fork/blob/master/src/libmojave/lm_instrument.mli&quot;&gt;Lm_instrument&lt;/a&gt;. (Note that
I did all the actual instrumentation in the &amp;quot;perf-test&amp;quot; branch.)

&lt;p&gt;As OCaml supports gprof instrumentation I also tried this but
without success. The problem is simply that gprof looks at the wrong
metrics, namely only at the runtimes of the two innermost function
invocations in the call stack. In OCaml this is usually something like
&lt;code&gt;List.map&lt;/code&gt; calling &lt;code&gt;String.sub&lt;/code&gt;, i.e. at both
levels there are general-purpose functions. This is useless
information. We need more context for the analysis (i.e. more levels
in the call stack), but it depends very much from where the function
is called.

&lt;/p&gt;&lt;p&gt;Another problem of gprof was that you do not see kernel time. For
analyzing a utility like OMake whose purpose is to start external
commands this is crucial information, though.

&lt;/p&gt;&lt;p&gt;For measuring the size of OCaml values I used &lt;a href=&quot;http://forge.ocamlcore.org/projects/objsize/&quot;&gt;objsize&lt;/a&gt;.


&lt;/p&gt;&lt;h2&gt;The main points of the improvement&lt;/h2&gt;

&lt;p&gt;Summarized, the following improvements were done:

&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;For Linux, I switched to posix_spawn instead of fork/exec for
    starting commands.
&lt;/li&gt;&lt;li&gt;For Linux, it was also important to avoid a self-fork of omake for
    postprocessing ocamldep output. Now temporary files are used.
&lt;/li&gt;&lt;li&gt;I rewrote the target cache that stores whether a file can be built
    or not. The new data structure for this cache highly compresses
    the data, and is better aligned to the main user, namely the
    function figuring out which implicit rules are needed to build
    a file. This way I could save processing time in this cache,
    and the memory footprint also got substantially smaller.
&lt;/li&gt;&lt;li&gt;I also rewrote the file cache that connects file names with file stats
    and digests. The new cache allows it to skip the computation of
    digests in more cases. Also, less data is cached (saving memory).
&lt;/li&gt;&lt;li&gt;I tweaked when the file digests are computed. This is no longer done
    immediately but delayed after the next command has been started,
    and in parallel to the command. This is in particular advantageous
    when there are some CPU resources left that could be utilized for
    this purpose.
&lt;/li&gt;&lt;li&gt;There are also simplified scanner rules in OMake.om, reducing the
    time needed for computing scanner dependencies. There is a drawback
    of the new rules, namely that when a file is moved to a new directory
    OMake does not rescan the file the next time it is run. I guess this is
    acceptable, because it normally does not matter where a file is
    stored. Nevertheless, there is an option to get the old behavior
    back (by setting EXTENDED_DIGESTS).
&lt;/li&gt;&lt;li&gt;Not regarding speed: OMake can now be built with the mingw port of OCaml
&lt;/li&gt;&lt;/ul&gt;


&lt;h2&gt;One major problem remains&lt;/h2&gt;

&lt;p&gt;
There is still one problem I could not yet address, and this problem is
mainly responsible for the long startup time of OMake for large builds.
Unlike other build systems, OMake creates a dependency from the rule
to the command of the rule, as if every rule looked like:

&lt;/p&gt;&lt;blockquote&gt;
&lt;code&gt;
target: source1 ... sourceN :value: $(command)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;$(command)
&lt;/code&gt;
&lt;/blockquote&gt;

i.e. when the command changes the rule &amp;quot;fires&amp;quot; and is executed. This is
an automatic addition, and it is very useful: When you start a build after
changing parameters (e.g. include paths) OMake automatically
detects which commands have changed because of this, and reruns these.

&lt;p&gt;
However, there is a price to pay. For checking whether a rule is out of date
it is required to expand the command and compute the digest. For a full
build the time for this is negligible (and you need the commands anyway
for starting them), but for a &amp;quot;zero rebuild&amp;quot; the commands are finally
not needed, and OMake expands them only for the out-of-date check. As you
might guess, this is the main reason why a zero rebuild is so slow.

&lt;/p&gt;&lt;p&gt;
It is probably possible to speed up the out-of-date check by doing a
static analysis of the command expansions. Most expansions just depend
on a small number of variables, and only if these variables change the
command can expand to something different. With that knowledge it is 
possible to compile a quick check whether the expansion is actually needed.
As any expression of the OMake language can be used for the commands,
developing such a compiler is non-trivial, and it was so far not possible
to do in my time budget.

&lt;/p&gt;&lt;div style=&quot;border: 1px solid black; padding: 10px; margin-left: 1em; margin-bottom: 1em; background-color: #E0E0E0&quot;&gt;
The next part will be published on Friday, 6/19.
&lt;/div&gt;

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/omake1_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;


&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as OCaml consultant.

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/omake1.html</id><title type="text">OMake On Steroids (Part 1)</title><updated>2015-06-16T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/bytes1.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;Why the concept is not good enough&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
In the upcoming release 4.02 of the OCaml programming language, the type
&lt;code&gt;string&lt;/code&gt; can be made immutable by a compiler
switch. Although this won't be the default yet, this should be seen as
the announcement of a quite disruptive change in the
language. Eventually this will be the default in a future version. In
this article I explain why I disagree with this particular plan, and
which modifications would be better.

&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;
Of course, the fact that &lt;code&gt;string&lt;/code&gt; is mutable doesn't fit
well into a functional language. Nevertheless, it has been seen as
acceptable for a long time, probably because the developers of OCaml
did not pay much attention to strings, and felt that the benefits of a
somewhat cleaner concept wouldn't outweigh the practical disadvantages
of immutable strings. Apparently, this attitude changed, and we will
see a new &lt;code&gt;bytes&lt;/code&gt; type in OCaml-4.02. This type is
accompanied by a &lt;code&gt;Bytes&lt;/code&gt; module with library functions
supporting it. The compiler was also extended so
that &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt; can be used
interchangably by default. If, however, the &lt;code&gt;-safe-strings&lt;/code&gt;
switch is set on the command-line, the compiler
sees &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt; as two completely
separate types.
&lt;/p&gt;

&lt;p&gt;
This is a disruptive change (if enabled): Almost all code bases will
need modifications in order to be compatible with the new
concept. Although this will often be trivial, there are also harder
cases where strings are frequently used as buffers. Before discussing
that a bit more in detail, let me point out why such disruptive
changes are so problematic. So far there was an implicit guarantee
that your code will be compatible to new compiler versions if you
stick to the well-established parts of the language and avoid
experimental additions.  I have in deed code that was developed for
OCaml-1.03 (the first version I checked out), and that code still
runs. Especially in a commercial context this is a highly appreciated
feature, because this protects the investment in the code base. As I'm
trying to sell OCaml to companies in my carreer this is a point that
bothers me. Giving up this history of excellent backward compatibility
is something we shouldn't do easily, and if so, only if we get something
highly valuable back. (Of course, if you only look at the open source
and academic use of OCaml, you'll put less emphasis on the compatibility
point, but it's also not completely unimportant there.)
&lt;/p&gt;


&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;
I'm fully aware that immutable strings fix some problems (the
worst probably: so far even string literals can be mutated, which can be
very surprising). However, creating a completely new type &lt;code&gt;bytes&lt;/code&gt;
comes also with some disadvantages:

&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Lack of generic accessor functions: There is &lt;code&gt;String.get&lt;/code&gt; and
there is &lt;code&gt;Bytes.get&lt;/code&gt;. The shorthand &lt;code&gt;s.[k]&lt;/code&gt; is now
restricted to strings. This is mostly a stylistic problem.

&lt;/li&gt;&lt;li&gt;The conversion of string to bytes and vice versa requires a copy:
&lt;code&gt;Bytes.of_string&lt;/code&gt;, and &lt;code&gt;Bytes.to_string&lt;/code&gt;. You have
to pay a performance penalty.

&lt;/li&gt;&lt;li&gt;In practical programming, there is sometimes no clear conceptual 
distinction between string data that are read-only and those that require
mutation. For example, if you add data to a buffer, the data may come from
a string or from another buffer. So how do you type such an &lt;code&gt;add&lt;/code&gt;
function?
&lt;/li&gt;&lt;/ul&gt;

This latter point is, in my opinion, the biggest problem. Let's assume
we wanted to reimplement the &lt;code&gt;Lexing&lt;/code&gt; module of the
standard library in pure OCaml without resorting to unsafe coding
(currently it's done in C). This module implements the lexing buffer
that backs the lexers generated with ocamllex. We now have to
use &lt;code&gt;bytes&lt;/code&gt; for the core of this buffer. There are three
functions in &lt;code&gt;Lexing&lt;/code&gt; for creating new buffers:

&lt;pre&gt;
val from_channel : in_channel -&amp;gt; lexbuf
val from_string : string -&amp;gt; lexbuf
val from_function : (string -&amp;gt; int -&amp;gt; int) -&amp;gt; lexbuf
&lt;/pre&gt;

The first observation is that we'll better offer two more constructors
to the users of this module:

&lt;pre&gt;
val from_bytes : bytes -&amp;gt; lexbuf
val from_bytes_function : (bytes -&amp;gt; int -&amp;gt; int) -&amp;gt; lexbuf
&lt;/pre&gt;

So why do we need the ability to read from &lt;code&gt;bytes&lt;/code&gt;,
i.e. copy from one buffer to the other? We could just be a bad host
and don't offer these functions to the users of the module. However,
it's unavoidable anyway for &lt;code&gt;from_channel&lt;/code&gt;, because I/O
buffers are of course &lt;code&gt;bytes&lt;/code&gt;:

&lt;pre&gt;
let from_channel ch =
  from_bytes_function (Pervasives.input ch)
&lt;/pre&gt;

So whenever we implement buffers that also include I/O capabilities,
it is likely that we need to handle both the &lt;code&gt;bytes&lt;/code&gt; and
the &lt;code&gt;string&lt;/code&gt; case. This is not only a problem for the
interface design. Because &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;
are completely separated, we need two different
implementations: &lt;code&gt;from_string&lt;/code&gt; and
&lt;code&gt;from_bytes&lt;/code&gt; cannot share much code.


&lt;p&gt;
This is the ironical part of the new concept: Although it tries to
make the handling of strings more sound and safe, the immediate
consequence in reality is that code needs to be duplicated because of
missing polymorphisms. Any half-way intelligent programmer will of
course fall back to unsafe functions for casting bytes to strings and
vice versa (&lt;code&gt;Bytes.unsafe_to_string&lt;/code&gt;
and &lt;code&gt;Bytes.unsafe_of_string&lt;/code&gt;), and this only means
that the new &lt;code&gt;-safe-strings&lt;/code&gt; option will be a driving force
for using unsafe language features.
&lt;/p&gt;

&lt;p&gt;
Let's look at three modifications of the concept. Is there some easy
fix?
&lt;/p&gt;

&lt;h2&gt;Idea 1: &lt;code&gt;string&lt;/code&gt; as a supertype of &lt;code&gt;bytes&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;
We just allow that &lt;code&gt;bytes&lt;/code&gt; can officially be
coerced to &lt;code&gt;string&lt;/code&gt;:
&lt;/p&gt;

&lt;pre&gt;
let s = (b : bytes :&amp;gt; string)
&lt;/pre&gt;

&lt;p&gt;
Of course, this weakens the immutability property: &lt;code&gt;string&lt;/code&gt;
may now be a read-only interface for a &lt;code&gt;bytes&lt;/code&gt; buffer, and
this buffer can be mutated, and this mutation can be observed through
the &lt;code&gt;string&lt;/code&gt; type:
&lt;/p&gt;

&lt;pre&gt;
let mutable_string() =
  let b = Bytes.make 1 'X' in
  let s = (b :&amp;gt; string) in
  (s, Bytes.set 0)

let (s, set) = mutable_string()
(* s is now &amp;quot;X&amp;quot; *)
let () = set 'Y'
(* s is now &amp;quot;Y&amp;quot; *)
&lt;/pre&gt;

&lt;p&gt;
Nevertheless, this concept is not meaningless. In particular, if a
function takes a string argument, it is guaranteed that the string
isn't modified. Also, string literals are immutable. Only when a
function returns a string, we cannot be sure that the string isn't
modified by a side effect.
&lt;/p&gt;

&lt;p&gt;
This variation of the concept also solves the polymorphism problem we
explained at the example of the &lt;code&gt;Lexing&lt;/code&gt; module: It is now
sufficient when we implement &lt;code&gt;Lexing.from_string&lt;/code&gt;, because
&lt;code&gt;bytes&lt;/code&gt; can always be coerced to &lt;code&gt;string&lt;/code&gt;:

&lt;/p&gt;&lt;pre&gt;
let from_bytes s =
  from_string (s :&amp;gt; string)
&lt;/pre&gt;


&lt;h2&gt;Idea 2: Add a read-only type &lt;code&gt;stringlike&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;
Some people may feel uncomfortable with the implication of Idea 1 that
the immutability of &lt;code&gt;string&lt;/code&gt; can be easily circumvented.
This can be avoided with a variation: Add a third type
&lt;code&gt;stringlike&lt;/code&gt; as the common supertype of both
&lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;. So we allow:

&lt;/p&gt;&lt;pre&gt;
let sl1 = (s : string :&amp;gt; stringlike)
let sl2 = (b : bytes :&amp;gt; stringlike)
&lt;/pre&gt;

Of course, &lt;code&gt;stringlike&lt;/code&gt; doesn't implement mutators (like
&lt;code&gt;string&lt;/code&gt;). It is nevertheless different from &lt;code&gt;string&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;string&lt;/code&gt; is considered as absolutely immutable (there is no
way to coerce &lt;code&gt;bytes&lt;/code&gt; to &lt;code&gt;string&lt;/code&gt;)
&lt;/li&gt;&lt;li&gt;&lt;code&gt;stringlike&lt;/code&gt; is seen as the read-only API for either
&lt;code&gt;string&lt;/code&gt; or &lt;code&gt;bytes&lt;/code&gt;, and it is allowed to mutate
a &lt;code&gt;stringlike&lt;/code&gt; behind the back of this API
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
&lt;code&gt;stringlike&lt;/code&gt; is especially interesting for interfaces that
need to be compatible to both &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;.
In the &lt;code&gt;Lexing&lt;/code&gt; example, we would just define

&lt;/p&gt;&lt;pre&gt;
val from_stringlike : stringlike -&amp;gt; lexbuf
val from_stringlike_function : (stringlike -&amp;gt; int -&amp;gt; int) -&amp;gt; lexbuf
&lt;/pre&gt;

and then reduce the other constructors to just these two, e.g.

&lt;pre&gt;
let from_string s =
  from_stringlike (s :&amp;gt; stringlike)

let from_bytes b =
  from_stringlike (b :&amp;gt; bytes)
&lt;/pre&gt;

These other constructors are now only defined for the convenience
of the user.

&lt;h2&gt;Idea 3: Base &lt;code&gt;bytes&lt;/code&gt; on bigarrays&lt;/h2&gt;

&lt;p&gt;
This idea doesn't fix any of the mentioned problems. Instead, the
thinking is: If we already accept the incompatibility
between &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;, let's at least do
in a way so that we get the maximum out of it. Especially for I/O
buffers, bigarrays are way better suited than strings:

&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;I/O primitives can directly pass the bigarrays to the operating
system (no need for an intermediate buffer as it is currently the case
for &lt;code&gt;Unix.read&lt;/code&gt; and &lt;code&gt;Unix.write&lt;/code&gt;)

&lt;/li&gt;&lt;li&gt;Bigarrays support the slicing of buffers (i.e. you can reference
subbuffers directly)

&lt;/li&gt;&lt;li&gt;Bigarrays can be aligned to page boundaries (which is accelerated
for some operating systems when used for I/O)
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
So let's define:

&lt;/p&gt;&lt;pre&gt;
type bytes =
  (char,Bigarray.int8_unsigned_elt,Bigarray.c_layout) Bigarray.Array1.t
&lt;/pre&gt;

Sure, there is now no way to unsafely cast strings to bytes and vice
versa anymore, but arguably we shouldn't prefer a design over the other
only for it's unsafety.


&lt;p&gt;
Regarding &lt;code&gt;stringlike&lt;/code&gt;, it is in deed possible to define it,
but there is some runtime cost. As &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;
have now different representations, any accessor function for 
&lt;code&gt;stringlike&lt;/code&gt; would have to check at runtime whether it is
backed by a &lt;code&gt;string&lt;/code&gt; or by &lt;code&gt;bytes&lt;/code&gt;. At least, this
check is very cheap.
&lt;/p&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;

I hope it has become clear that the current plan is not far reaching
enough, as the programmer would have to choose between bad alternatives:
either pay a runtime penalty for additional copying and accept that
some code needs to be duplicated, or use unsafe coercion
between &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;. The latter is not
desirable, of course, but it is surely the task of the language
(designer) to make sound and safe string handling an attractive option.
I've presented three ideas that would all improve the concept in
some respect. In particular, the combination of the ideas 2 and 3
seems to be very attractive: back &lt;code&gt;bytes&lt;/code&gt; by bigarrays,
and provide an &lt;code&gt;stringlike&lt;/code&gt; supertype for easing the
programming of application buffers.

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/bytes1_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;


&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/bytes1.html</id><title type="text">Immutable strings in OCaml-4.02</title><updated>2014-07-04T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/godi_shutdown.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;Sorry!&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
Unfortunately, it is no longer possible for me to run the GODI
distribution. GODI will not upgrade to OCaml 4.01 once it is out,
and it will shut down the public service in the course of September 2013.

&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;This website, camlcity.org, will remain up, but with reduced
content. Existing GODI installations can be continued to be used,
but upgrades or bugfixes will not be available when GODI is off.

&lt;/p&gt;&lt;p&gt;
Although there are still a lot of GODI users, it is unavoidable
to shut GODI down due to lack of supporters, especially package
developers. I was more or less alone in the past months, and my
time contingent will not allow it to do the upgrade to OCaml 4.01
alone (when it is released).

&lt;/p&gt;&lt;p&gt;
Also, there was a lot of noise about a competing packaging system
for OCaml in the past weeks: OPAM. Apparently, it got a lot of
attention both from individuals and from organizations. As I see
it, the OCaml community is too small to support two systems, and
so in some sense GODI is displaced by OPAM.

&lt;/p&gt;&lt;p&gt;
The sad part is that OPAM is only clearly better in one point,
namely in interacting with the community (via Github). In times
where social networks are worth billions this is probably the
striking point. It doesn't matter that OPAM lacks
some features GODI has.
So there is some loss of functionality for the community
(partly difficult to replace, like GODI's support for Windows).

&lt;/p&gt;&lt;p&gt;
If somebody wants to take over GODI, please do so. The 
&lt;a href=&quot;https://godirepo.camlcity.org/svn/godi-bootstrap/&quot;&gt;source code&lt;/a&gt;
is still available as well as the 
&lt;a href=&quot;https://godirepo.camlcity.org/svn/godi-build/&quot;&gt;package directories&lt;/a&gt;.
Maybe it is sufficient to move the repository to a public place and to
redesign the package release process to give GODI a restart.

&lt;/p&gt;&lt;p&gt;
Hoorn (NL), the 22nd July 2013,

&lt;/p&gt;&lt;p&gt;
Gerd Stolpmann
&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/godi_shutdown.html</id><title type="text">GODI is shutting down</title><updated>2013-07-22T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/ipv6.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;camlcity.org now connected&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
For two weeks the camlcity.org website is fully connected to IPv6.

&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;
Actually, the raw connectivity exists already for more than two years,
but I haven't found time to put the IP addresses into DNS. This is now
done, making the site visible.
&lt;/p&gt;

&lt;p&gt;
Around 1% of the traffic is now via IPv6. This is way more than I was
expecting. Here in Germany, only a few Internet providers have already
rolled out IPv6, but the major players are planning it for 2014. It
turns out that at home I already have IPv6, although only via
DSLite. (NB. In the default DNS configuration a client connected with
DSLite or other 6-in-4 technologies will pick the IPv4 address if both
&amp;quot;Internets&amp;quot; are available, so such clients will not show up in my web
server logs as IPv6.)
&lt;/p&gt;

&lt;p&gt;
The IPv6 world is different: no NAT anymore, and every computer
has a globally routable address. This is something you need to get
used to - the Internet appears again as a real peer-to-peer
network as in the first years, and the distinction between client
and datacenter connectivity is gone. Let's hope this drives
innovation - like user-controlled social networks, for instance.
&lt;/p&gt;


&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/ipv6.html</id><title type="text">Welcome IPv6</title><updated>2013-06-21T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/plasma6.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;A performance test&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
Last week I spent some time running map/reduce jobs on Amazon EC2.
In particular, I compared the performance of Plasma, my own map/reduce
implementation, with Hadoop. I just wanted to know how much my implementation
was behind the most popular map/reduce framework. However, the suprise was
that Plasma turned out as slightly faster in this setup.

&lt;/div&gt;

&lt;div&gt;
  
&lt;div style=&quot;float:right; width: 50ex; font-size:small; color:grey; border: 1px solid grey; padding: 1ex; margin-left: 2ex&quot;&gt;
This article is also available in other languages:
&lt;dl&gt;
&lt;dt&gt;&lt;a href=&quot;http://science.webhostinggeeks.com/plasma-map-reduce&quot;&gt;[Serbo-Croatian]&lt;/a&gt;
&lt;/dt&gt;&lt;dd&gt;translation by Anja Skrba from 
&lt;a href=&quot;http://webhostinggeeks.com/&quot;&gt;Webhostinggeeks.com&lt;/a&gt;
&lt;/dd&gt;&lt;/dl&gt;
&lt;/div&gt;
&lt;p&gt;
I would not call this test a &amp;quot;benchmark&amp;quot;. Amazon EC2 is not a
controlled environment, as you always only get partial machines, and
you don't know how much resources are consumed by other users on the
same machines.  Also, you cannot be sure how far the nodes are off
from each other in the network. Finally, there are some special
effects coming from the virtualization technology, especially the
first write of a disk block is slower (roughly half the normal speed)
than following writes.  However, EC2 is good enough to get an
impression of the speed, and one can hope that all the test runs
get the same handicap on average.

&lt;/p&gt;&lt;p&gt;
The task was to sort 100G of data, given in 10 files. Each line has
100 bytes, divided into a key of 8 bytes, a TAB character, 90 random
bytes as value, and an LF character. The key was randomly chosen from
65536 possible values. This means that there were lots of lines with
the same key - a scenario where I think it is more typical of map/reduce
than having unique keys. The output is partitioned into 80 sets.

&lt;/p&gt;&lt;p&gt;
I allocated 1 larger node (m1-xlarge) with 4 virtual cores and 15G of
RAM acting as combined name- and datanode, and 9 smaller nodes
(m1-large) with 2 virtual cores and 7.5G of RAM for the other
datanodes. Each node had access to two virtual disks that were
configured as RAID-0 array. The speed for sequential reading or
writing was around 160 MB/s for the array (but only 80 MB/s for the
first time blocks were written). Apparently, the nodes had Gigabit
network cards (the maximum transfer speed was around 119MB/s).

&lt;/p&gt;&lt;p&gt;
During the tests, I monitored the system activity with the sar utility.
I observed significant cycle stealing (meaning that a virtual core is
blocked because there is no free real core), often reaching values of
25%. This could be interpreted as overdriving the available resources,
but another explanation is that the hypervisor needed this time for
itself. Anyway, this effect also questions the reliability of this
test.

&lt;/p&gt;&lt;h2&gt;The contrahents&lt;/h2&gt;

&lt;p&gt;
Hadoop is the top dog in the map/reduce scene. In this test, the
version from Cloudera 0.20.2-cdh3u2 was used, which contains more than
1000 patches against the vanilla 0.20.2 version. Written in Java, it
needs a JVM at runtime, which was here IcedTea 1.9.10 distributing
OpenJDK 1.6.0_20. I did not do any tuning, hoping that the configuration
would be ok for a small job. The HDFS block size was 64M, without
replication.

&lt;/p&gt;&lt;p&gt;
The contender is Plasma Map/Reduce. I started this project two years
ago in my spare time. It is not a clone of the Hadoop architecture,
but includes many new ideas. In particular, a lot of work went into
the distributed filesystem PlasmaFS which features an almost complete
set of file operations, and controls the disk layout directly. The
map/reduce algorithm uses a slightly different scheme which tries
to delay the partitioning of the data to get larger intermediate files.
Plasma is implemented in OCaml, which isn't VM-based but compiles
the code directly to assembly language. In this test, the blocksize
was 1M (Plasma is designed for smaller-sized blocks). The software
version of Plasma is roughly 0.6 (a few svn revisions before the release
of 0.6).

&lt;/p&gt;&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;The runtimes:

&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Hadoop:&lt;/b&gt;&lt;/td&gt;     &lt;td&gt;&lt;b&gt;2265 seconds&lt;/b&gt; (37 min, 45 s)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Plasma:&lt;/b&gt;&lt;/td&gt;     &lt;td&gt;&lt;b&gt;1975 seconds&lt;/b&gt; (32 min. 55 s)&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;
Given the uncertainty of the environment, this is no big difference.
But let's have a closer look at the system activity to get an idea
why Plasma is a bit faster.

&lt;/p&gt;&lt;h2&gt;CPU&lt;/h2&gt;

In the following I took simply one of the datanodes, and created
diagrams (with kSar):

&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_hadoop_cpu_all.png&quot; width=&quot;799&quot; height=&quot;472&quot;/&gt;

&lt;/p&gt;&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_plasma_cpu_all.png&quot; width=&quot;800&quot; height=&quot;471&quot;/&gt;

&lt;/p&gt;&lt;p&gt;
Note that kSar does not draw graphs for %iowait and %steal, although 
these data are recorded by sar. This is the explanation why the sum of
user, system and idle is not 100%. 

&lt;/p&gt;&lt;p&gt;
What we see here is that Hadoop consumes all CPU cycles, whereas
Plasma leaves around 1/3 of the CPU capacity unused. Given the fact
that this kind of job is normally I/O-bound, it just means that Hadoop
is more CPU-hungry, and would have benefit from getting more cores
in this test.

&lt;/p&gt;&lt;h2&gt;Network&lt;/h2&gt;

In this diagram, reads are blue and red, whereas writes are green and
black. The first curve shows packets per second, and the second bytes
per second:

&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_hadoop_eth0.png&quot; width=&quot;800&quot; height=&quot;333&quot;/&gt;

&lt;/p&gt;&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_plasma_eth0.png&quot; width=&quot;800&quot; height=&quot;319&quot;/&gt;

Summing reads and writes up, Hadoop uses only around 7MB/s on average
whereas Plasma transmits around 25MB/s, more than three times as
much. There could be two explanations:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;Because Hadoop is CPU-underpowered, it remains below its
      possibilities
  &lt;/li&gt;&lt;li&gt;The Hadoop scheme is more optimized for keeping the network
      bandwidth as low as possible
&lt;/li&gt;&lt;/ul&gt;

The background for the second point is the following: Because Hadoop
partitions the data immediately after mapping and sorting, the data
has (ideally) only to cross the network once.  This is different in
Plasma - which generally partitions the data iteratively. In this
setup, after mapping and sorting only 4 partitions are created, which
are further refined in the following split-and-merge rounds.  As we
have here 80 partitions in total, there is at least one further step
in which data partitioning is refined, meaning that the data has to
cross the network roughly twice. This already explains 2/3 of the
observed difference.  (As a side note, one can configure how many
partitions are initially created after mapping and sorting, and it
would have been possible to mimick Hadoop's scheme by setting this
value to 80.)

&lt;h2&gt;Disks&lt;/h2&gt;

These diagrams depict the disk reads and writes in KB/second:

&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_hadoop_md0.png&quot; width=&quot;800&quot; height=&quot;332&quot;/&gt;

&lt;/p&gt;&lt;p&gt;
&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/edited_plasma_md0.png&quot; width=&quot;800&quot; height=&quot;332&quot;/&gt;

The average numbers are (directly taken from sar):

&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;th&gt;Hadoop&lt;/th&gt;
    &lt;th&gt;Plasma&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Read/s:&lt;/td&gt;
    &lt;td&gt;17.6 MB/s&lt;/td&gt;
    &lt;td&gt;31.2 MB/s&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Write/s:&lt;/td&gt;
    &lt;td&gt;30.8 MB/s&lt;/td&gt;
    &lt;td&gt;33.9 MB/s&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;
Obviously, Plasma reads data around twice as often from disk than
Hadoop, whereas the write speed is about the same. Apart from this, it
is interesting that the shape of the curves are quite different:
Hadoop has a period of high disk activity at the end of the job (when
it is busy merging data), whereas Plasma utilizes the disks better
during the first third of the job.

&lt;/p&gt;&lt;h2&gt;Plausibility&lt;/h2&gt;

&lt;p&gt;
Neither of the contenders utilized the I/O resources at all times
best. Part of the difficulty of developing a map/reduce scheme is to
achieve that the load put onto the disks and onto the network is
balanced. It is not good when e,g, the disks are used to 100% at a
certain point and the network is underutilized, but during the next
period the network is at 100% and the disk not fully used. A balanced
distribution of the load reaches higher throughput in total.

&lt;/p&gt;&lt;p&gt;
Let's analyze the Plasma scheme a bit more in detail. The data set of
100G (which does not change in volume during the processing) is copied
four times in total: once in the map-and-sort phase, and three times
in the reduce phase (for this volume Plasma needs three merging
rounds). This means we have to transfer 4 * 100G of data in total, or
40G of data per node (remember we have 10 nodes). We ran 22 cores for
1975 seconds, which gives a capacity of 43450 CPU seconds. Plasma
tells us in its reports that it used 3822 CPU seconds for in-RAM
sorting, which we should subtract for analyzing the I/O
throughput. Per core these are 173 seconds. This means each node had
1975-173 = 1802 seconds for handling the 40G of data. This makes
around 22 MB per second on each node.

&lt;/p&gt;&lt;p&gt;
The Hadoop scheme differs mostly in that the data is only copied twice
in the merge phase (because Hadoop by default merges more files in
one round than Plasma). However, because of its design there is an
extra copy at the end of the reduce phase (from disk to HDFS).  This
means Hadoop also solves the same job by transferring 4 * 100G of data.
There is no counter for measuring the time spent for in-RAM sorting.
Let's assume this time is also around 3800 seconds. This means each
node had 2265 - 175 = 2090 seconds for handling 40G of data, or
19 MB per second on each node.

&lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;
It looks very much as if both implementations are slowed down by
specifics of the EC2 environment. Especially the disk I/O, probably
the essential bottleneck here, is far below what one can expect.
Plasma probably won because it uses the CPU more efficiently, whereas
other aspects like network utilization are better handled by Hadoop.

&lt;/p&gt;&lt;p&gt;
For my project this result just means that it is on the right track.
Especially, this small setup (only 10 nodes) is easily handled, giving
prospect that Plasma is scalable at least to a small multitude of
this. The bottleneck would be here the namenode, but there is still a
lot of headroom.

&lt;/p&gt;&lt;h2&gt;Where to get Plasma&lt;/h2&gt;

&lt;p&gt;Plasma Map/Reduce and PlasmaFS are bundled together in one download. Here is the
&lt;a href=&quot;http://projects.camlcity.org/projects/plasma.html&quot;&gt;project page&lt;/a&gt;.

&lt;/p&gt;&lt;p&gt;

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/plasma6_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;

&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/plasma6.html</id><title type="text">Plasma Map/Reduce Slightly Faster Than Hadoop</title><updated>2012-02-01T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/plasma5.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;An experiment, and a vision&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;
The recent success of NoSQL technologies has not only to do with the
fact that it is taken advantage of distribution and replication, but
even more with the &amp;quot;middleware effect&amp;quot; that these features became
relatively easy to use.  Now it is no longer required to be an expert
for these cluster techniques in order to profit from them. Let's think
a bit ahead: how could a platform look like that makes distributed
programming even easier, and that integrates several styles of storing
data and managing computations?

&lt;cc-field name=&quot;maintext&quot;&gt;
&lt;p&gt;
The starting point for this exploration is a recent experience I made
with my own attempt in the NoSQL arena,
the &lt;a href=&quot;http://plasma.camlcity.org&quot;&gt;Plasma project&lt;/a&gt;. Two weeks
ago, it was &amp;quot;only&amp;quot; a distributed, replicating, and failure-resiliant
filesystem PlasmaFS, with its own map/reduce implementation on top of
it. Then I had an idea: is it possible to develop a key/value database
on top of this filesystem? Which features, and relative
advantages/disadvantages would it have? In other words, I was
examining whether the existing platform makes it simpler to develop
a database with a reasonable feature set.

&lt;/p&gt;&lt;p&gt;
When we talk about clusters, I have especially Internet applications
in mind that are bombarded by the users with requests, but that have
also to do a lot of background processing.


&lt;/p&gt;&lt;h2&gt;The key/value database needed less than 2000 lines of code&lt;/h2&gt;

&lt;p&gt;
Now, PlasmaFS is not following the simple pattern of HDFS, but bases
on a transactional core, and it even allows the users to manage the
transactions. For example, it is possible to rename a bunch of files
atomically by just wrapping the rename operations into a single
transaction.  The transactional support goes even further: When
reading from a file one can activate a special snapshot mode, which
just means that the reader's view of the file is isolated from any
writes happening at the same time.

&lt;/p&gt;&lt;p&gt;
These are clearly advanced features, and the question was whether they
helped for writing a key/value database library. And yes, it was
extremely helpful - in less than 2000 lines of code this library
provides data distribution and replication, a high degree of data
safety, almost unlimited scalabilitiy for database reads, and
reasonable performance for writes. Of course, most of these features
are just &amp;quot;inherited&amp;quot; from PlasmaFS, and the library just had to
implement the file format (i.e. a B tree,
see &lt;a href=&quot;http://projects.camlcity.org/projects/dl/plasma-0.5/doc/html/Plasmakv_intro.html&quot;&gt;
this page for details&lt;/a&gt;). This is not cheating, but exactly the
point: the platform makes it easy to provide features that would
otherwise be extremely complicated to provide.

&lt;/p&gt;&lt;h2&gt;NoServer&lt;/h2&gt;

&lt;p&gt;
This key/value database is just a library, and one can use it only
on machines where PlasmaFS is deployed. Of course it is possible to
access the same database file from several machines - PlasmaFS handles
all the networking involved with it. The point is that during the
implementation of the library this never had to be taken into account.
There is no networking code in this library, and this is why it is
the first example of the new NoServer paradigm - not only server.

&lt;/p&gt;&lt;p&gt;
The genuine advantage of this paradigm is that it enables developers
to write code they never would be able to create without the help of
the platform. This is a bit comparable to the current situation for
SQL databases: Everybody can store data in them, even over the
network, without needing to have any clue how this works in detail.
In the NoServer paradigm, we just go one step further, because the
provided services by the platform are a lot more low-level, and the
developer has a lot more freedom. Instead with a query language
the shared resources are accessed with normal file operations,
extended by transactional directives. The hope is that this makes
a lot of server programming superflous, especially the difficult
parts of it (e.g. what to do when a machine crashes).

&lt;/p&gt;&lt;p&gt;
A simple key/value database is obviously not difficult to create with
these programming means. The interesting question is what else can be
done with it in a cluster environment. Obviously, having a common
filesystem on all machines of the cluster makes a lot of file copying
superflous that a normal cluster would do with rsync and/or
ssh. PlasmaFS can even be directly mounted (although the transactional
features are unavailable then), so even applications can access
PlasmaFS files that have not specially been ported to it.  An example
would be a read-only Lucene search index residing in PlasmaFS.
Replacing the index by an updated one would be done by simply moving
the new index into the right directory, and signalling Lucene that it
has to re-open the index.

&lt;/p&gt;&lt;p&gt;
So far Plasma is implemented, and works well (I just released the
release 0.5, which is now beta quality). The vision goes of course
beyond that.

&lt;/p&gt;&lt;h2&gt;What the platform also needs&lt;/h2&gt;

&lt;p&gt;
There are a number of further datastructures that can obviously be
well represented in files, such as hashtables or queues. Let's explore
the latter a bit more in detail: How would a queue manager look like?
There are a few data representation options. For example, every queue
element could be a file in a directory, or a container format is
established where the elements can be appended to. PlasmsFS also
allows it to cut arbitrary holes into files, so it is even possible to
physically remove elements from the beginning of the queue file by
just removing the data blocks storing the elements from the file.  As
we don't want to run the queue manager as server, but just as library
inside any program accessing the queue, the question is how event
notifications are handled (which would be obvious in server context).
Usually, one has to notify some followup processor when new elements
have been added to the queue. Plasma currently does not include a
method for doing this, so the platform needs to be extended by a
notification framework (which should not be too difficult).

&lt;/p&gt;&lt;p&gt;
An important question is also how programs are activated running on
different nodes. In my vision there would be a central task execution
manager. Of course, this manager is normal client/server middleware.
Again, the point here is that the application developer needs no 
special skills for triggering remote activation, he just uses
libraries. I've no absolutely clear picture of this part yet, but
it seems to be necessary to have the option of invoking programs
in the inetd style as well as directly as if started via ssh.
Also, a central directory would be maintained that includes
important data such as which program can be run on which node.

&lt;/p&gt;&lt;h2&gt;We won't live totally without servers, only with fewer ones&lt;/h2&gt;

&lt;p&gt;
My vision does not include that servers are completely banned. We will
still need them for special features or data access patterns, and of
course for interaction with other systems.  For example, PlasmaFS is
bad at coordinating concurrent write accesses to the same file. Also,
PlasmaFS employs a central namenode with a limited capacity only. So,
if you are doing OLTP processing, a normal SQL database will still do
better. If you need extraordinary write performance, but can pay the
price of weakened consistency guarantees, a system like Cassandra will
work better.

&lt;/p&gt;&lt;p&gt;
Nevertheless, there is the big field of &amp;quot;average deployments&amp;quot; where
the number of nodes is not too big and the performance requirements
are not too special, but the ACID guarantees PlasmaFS gives are
essential. For this field, the NoServer paradigm could be the ideal
choice to reduce the development overhead dramatically.

&lt;/p&gt;&lt;h2&gt;Check Plasma out&lt;/h2&gt;

The &lt;a href=&quot;http://plasma.camlcity.org&quot;&gt;Plasma homepage&lt;/a&gt; provides
a lot of documentation, and especially downloads. Also take a look at
the &lt;a href=&quot;http://plasma.camlcity.org/plasma/perf.html&quot;&gt;performance
page&lt;/a&gt;, describing a few tests I recently ran.

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/plasma5_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;



&lt;/cc-field&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant.
&lt;a href=&quot;http://blog.camlcity.org/blog/search1.html&quot;&gt;Currently looking for new jobs as consultant!&lt;/a&gt;

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/plasma5.html</id><title type="text">After NoSQL there will be NoServer</title><updated>2011-11-04T00:00:00-00:00</updated><author><name>camlcity</name></author></entry><entry><link href="http://blog.camlcity.org/blog/plasma4.html" rel="alternate"/><contributor><uri>http://blog.camlcity.org/blog/rss</uri><name>camlcity</name></contributor><content type="html">

&lt;div&gt;
  &lt;b&gt;A serious distributed filesystem&lt;/b&gt;&lt;br/&gt;&amp;nbsp;
&lt;/div&gt;

&lt;div&gt;
  
&lt;p&gt;
A few days ago, I
released &lt;a href=&quot;http://plasma.camlcity.org&quot;&gt;Plasma-0.4.1&lt;/a&gt;.  This
article gives an overview over the filesystem subsystem of it, which
is actually the more important part. PlasmaFS differs in many points
from popular distributed filesystems like HDFS. This starts from the
beginning with the requirements analysis.

&lt;cc-field name=&quot;maintext&quot;&gt;
&lt;p&gt;
A distributed filesystem (DFS) allows it to store giant amounts of
data.  A high number of data nodes (computers with hard disks) can be
attached to a DFS cluster, and usually a second kind of node, called
name node, is used to store metadata, i.e. which files are stored and
where. The point is now that the volume of metadata can be very low
compared to the payload data (the ratios are somewhere between
1:10,000 to 1:1,000,000), so a single name node can manage a quite
large cluster. Also, the clients can contact the data nodes
directly to access payload data - the traffic is not routed via
the name node like in &amp;quot;normal&amp;quot; network filesystems. This allows
enormous bandwidths.

&lt;/p&gt;&lt;p&gt;
The motivation for developing another DFS was that existing
implementations, and especially the popular HDFS, make (in my opinion)
unfortunate compromises to gain speed:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;The metadata is not well protected. Although the metadata is
   saved to disk and usually also replicated to another computer, these 
   &amp;quot;safety copies&amp;quot; lag behind. In the case of an outage, data loss
   is common (HDFS even fails fatally when the disk fills up).
   Given the amount of data, this is not acceptable. It's like a
   local filesystem without journaling.&lt;br/&gt;&amp;nbsp;
  &lt;/li&gt;&lt;li&gt;The name node protocol is too simplistic, and because of this,
   DFS implementations need ultra-high-speed name node implementations
   (at least several 10000 operations per second) to manage larger clusters.
   Another consequence is that only large block sizes (several megabytes)
   promise decent access speeds, because this is the only implemented
   strategy to reduce the frequency of name node operations.&lt;br/&gt;&amp;nbsp;
  &lt;/li&gt;&lt;li&gt;Unless you can physically separate the cluster from the rest
    of the network, security is a requirement. It is difficult to provide,
    however, mainly because the data nodes are independently accessed, and you
    want to avoid that data nodes have to continuously check for
    access permissions. So the compromise is to leave this out in the
    DFS, and rely on complicated and error-prone configurations in
    network hardware (routers and gateways).
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
I'm not saying that HDFS is a bad implementation. My point is only that
there is an alternative where safety and security are taken more
seriously, and that there are other ways to get high speed than those
that are implemented in HDFS.

&lt;/p&gt;&lt;h2&gt;Using SSDs for transacted metadata stores&lt;/h2&gt;

PlasmaFS starts at a different point. It uses a data store with full
transactional support (right now this is PostgreSQL, just for
development simplicity, but other, and more light-weight systems could
also fill out this role). This includes:

&lt;ul&gt;
  &lt;li&gt;Data are made persistent in a way so that full ACID support
    is guaranteed (remember, the ACID properties are atomicity,
    consistency, isolation, and durability).
  &lt;/li&gt;&lt;li&gt;For keeping replicas synchronized, we demand support for
    two-phase commit, i.e. that transactions can be prepared before
    the actual commit with the guarantee that the commit is fail-safe
    after preparation. (Essentially, two-phase commit is a protocol
    between two database systems keeping them always consistent.)
&lt;/li&gt;&lt;/ul&gt;

This is, by the way, the established prime-standard way of ensuring
data safety for databases.  It comes with its own problems, and the
most challenging is that commits are relatively slow. The reason for this
is the storage hardware - for normal hard disks the maximum frequency
of commits is a function of the rotation speed. Fortunately, there is
now an alternative: SSDs allow at present several 10000 syncs per
second, which is two orders of magnitude more than classic hard disks
provide. Good SSDs are still expensive, but luckily moderate disk
sizes are already sufficient (with only a 100G database you can
already manage a really giant filesystem).

&lt;p&gt;Still, writing each modification directly to the SSD limits the
speed compared to what systems like HDFS can do (because HDFS keeps
the data in RAM, and only writes now and then a copy to disk).  We need
more techniques to address the potential bottleneck name node:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;PlasmaFS provides a transactional view to users. This works
    very much like the transactions in SQL. The performance advantage is here that
    several write operations can be carried out with only one commit.
    PlasmaFS takes it that far that unlimited numbers of metadata
    operations can be put into a transaction, such as creating and
    deleting files, allocating blocks for the files, and retrieving
    block lists. It is possible to write terabytes of data to files with
    &lt;i&gt;only a single commit&lt;/i&gt;! Applications accessing large files
    sequentially (as, e.g., in the map/reduce framework) can especially
    profit from this scheme.&lt;br/&gt;&amp;nbsp;
  &lt;/li&gt;&lt;li&gt;PlasmaFS addresses blocks linearly: for each data node the blocks
    are identified by numbers from 0 to n-1. This is safe, because we
    manage the consistency globally (basically, there is a kind of
    join between the table managing which blocks are used or free, and
    the table managing the block lists per file, and our safety
    measures allow it to keep this join consistent). In contrast,
    other DFS use GUIDs to identify blocks. The linear scheme,
    however, allow it to transmit and store block lists in a
    compressed way (extent-based). For example, if a file uses the
    blocks 10 to 14 on a data nodes, this is stored as &amp;quot;10-14&amp;quot;, and not
    as &amp;quot;10,11,12,13,14&amp;quot;. Also, block allocations are always done
    for ranges of blocks. This greatly reduces the number
    of name node operations while only moderately increasing their
    complexity.&lt;br/&gt;&amp;nbsp;
  &lt;/li&gt;&lt;li&gt;A version number is maintained per file that is
    increased whenever data or metadata are modified. This allows it
    to keep external caches up to date with only low overhead: A quick
    check whether the version number has changed is sufficient to
    decide whether the cache needs to be refreshed. This is reliable,
    in contrast to cache consistency schemes that base only on the
    last modification time. Currently this is used to keep the
    caches of the NFS bridge synchronized. Especially, applications accessing
    only a few files randomly profit from such caching.
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
I consider the map/reduce part of Plasma especially as a good test
case for PlasmaFS. Of course, this map/reduce implementation is
perfectly adapted to PlasmaFS, and uses all possibilities to reduce
the frequency of name node operations. It turns out that a typical
running map/reduce task contacts the name node only every 3-4 seconds,
usually to refill a buffer that got empty, or to flush a full buffer
to disk. The point here is that a buffer can be larger than a data
block, and that only a single name node transaction is sufficient to
handle all blocks in the buffer in one go. The buffers are typically
way larger than only a single block, so this reduces the number of
name node operations quite dramatically.  (Important note: This number
(3-4) is only correct for Plasma's map/reduce implementation which
uses a modified and more complex algorithm scheme, but it is not
applicable to the scheme used by Hadoop.)

&lt;/p&gt;&lt;h2&gt;Speed&lt;/h2&gt;

&lt;p&gt;
I have done some tests with the latest development version of
Plasma. The peak number of commits per second seems to be around 500
(here, a &amp;quot;commit&amp;quot; is a transaction writing data that can include
several data update operations). This test used a recently bought SSD,
and ran on a quad-core server machine. It was not evident that the SSD
was the bottleneck (one indication is that the test ran only slightly
faster when syncs were turned off), so there is probably still a lot
of room for optimization.

&lt;/p&gt;&lt;p&gt;
Given that a map/reduce task needs the name node only every &amp;asymp;0.3 seconds,
this &amp;quot;commit speed&amp;quot; would be theoretically sufficient for around
1600 parallely running tasks. It is likely that other limits are
hit first (e.g. the switching capacity). Anyway, these are encouraging
numbers showing that this young project is not on the wrong track.

&lt;/p&gt;&lt;p&gt;
The above techniques are already implemented in PlasmaFS. More advanced
options that could be worth an implementation include:

&lt;/p&gt;&lt;ul&gt;
  &lt;li&gt;As we can maintain exact replicas of the primary name node (via
    two-phase commit), it becomes possible to also use the replicas
    for read accesses. For certain types of read operations this is
    non-trivial, though, because they have an effect on the block
    allocation map (essentially we would need to synchronize a certain
    buffer in both the primary and secondary servers that controls
    delayed block deallocation). nevertheless, this is certainly a viable option.
    Even writes could be handled by
    the secondary nodes, but this tends to become very complicated,
    and is probably not worth it.&lt;br/&gt;&amp;nbsp;
  &lt;/li&gt;&lt;li&gt;An easier option to increase the capacity is to split the file
    space, so that each name node takes care of a partition only. A
    user transaction would still need a uniform view on the filesystem,
    though. If a name node receives a request for an operation it
    cannot do itself, it automatically extends the scope of the
    transaction to the name node that is responsible for the right
    partition. This scheme would also use the two-phase commit protocol
    for keeping the partitions consistent. I think this option is viable,
    but only for the price of a complex development effort.
&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
Given that these two improvements are very complicated to implement,
it is unlikely that it is done soon. There is still a lot of fruit
hanging at lower branches of the tree.


&lt;/p&gt;&lt;h2&gt;Delegated access control checks&lt;/h2&gt;

&lt;p&gt;
Let's quickly discuss another problem, namely how to secure accesses
to data nodes. It is easy to accept that the name nodes can be secured
with classic authentication and authorization schemes in the same
style as they are used for other server software, too. For data nodes,
however, we face the problem that we need to supervise every access to a
data block individually, but want to avoid any extra overhead, especially
that each data access needs to be checked with the name node.

&lt;/p&gt;&lt;p&gt;
PlasmaFS uses a special cryptographic ticket system to avoid
this. Essentially, the name node creates random keys in periodical
intervals, and broadcasts these to the data nodes. These keys are
secrets shared by the name and data nodes. The accessing clients get
only HMAC-based tickets generated from the keys and from the block ID
the clients are granted access to.  These tickets can be checked by
the data nodes because these nodes know the keys. When the client
loses the right to access the blocks (i.e. when the client transaction
ends), the corresponding key is revoked.

&lt;/p&gt;&lt;p&gt;
With some additional tricks it can be achieved that the only
communication between the name node and the data node is a periodical
maintenance call that hands out the new keys and revokes the expired
keys. That's an acceptable overhead.


&lt;/p&gt;&lt;h2&gt;Other quality-assuring features&lt;/h2&gt;

&lt;p&gt;
PlasmaFS implements the POSIX file semantics almost completely. This
includes the possibility of modifying data (or better, replacing
blocks by newer versions, which is not possible in other DFS
implementations), the handling of deleted files, and the exclusive
creation of new files. There are a few exceptions, though, namely
neither the link count nor the last access time of files are maintained.
Also, lockf-style locks are not yet available.

&lt;/p&gt;&lt;p&gt;
For supporting map/reduce and other distributed algorithm schemes,
PlasmaFS offers locality functions. In particular, one can find out
on which nodes a data block is actually stored, and one can also
wish that a new data block is stored on a certain node (if possible).

&lt;/p&gt;&lt;p&gt;
The PlasmaFS client protocol bases on SunRPC. This protocol has quite
good support on the system level, and it supports strong
authentication and encryption via the GSS-API extension (which is
actually used by PlasmaFS, together with the SCRAM-SHA1 mechanism). I
know that younger developers consider it as out-dated, but even the
Facebook generation must accept that it can keep up with the
requirements of today, and that it includes features that more modern
protocols do not provide (like UDP transport and GSS-API). For the
quality of the code it is important that modifying the SunRPC layer is
easy (e.g. adding or changing a new procedure), and does not imply
much coding. Because of this it could be achieved that the PlasmaFS
protocol is quite clean on the one hand, but is still adequately
expressive on the other hand to support complex transactions.

&lt;/p&gt;&lt;p&gt;
PlasmaFS is accessible from many environments. Applications can access
it via the mentioned SunRPC protocol (with all features), but also
via NFS, and via a command-line client. In the future, WebDAV support
will also be provided (which is an extension of HTTP, and which will
ensure easy access from many programming environments).

&lt;/p&gt;&lt;h2&gt;Check Plasma out&lt;/h2&gt;

The &lt;a href=&quot;http://plasma.camlcity.org&quot;&gt;Plasma homepage&lt;/a&gt; provides
a lot of documentation, and especially downloads. Also take a look at
the &lt;a href=&quot;http://plasma.camlcity.org/plasma/perf.html&quot;&gt;performance
page&lt;/a&gt;, describing a few tests I recently ran.

&lt;img src=&quot;http://blog.camlcity.org/files/img/blog/plasma4_bug.gif&quot; width=&quot;1&quot; height=&quot;1&quot;/&gt;



&lt;/cc-field&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;

&lt;div&gt;
  Gerd Stolpmann works as O'Caml consultant.
&lt;a href=&quot;http://blog.camlcity.org/blog/search1.html&quot;&gt;Currently looking for new jobs as consultant!&lt;/a&gt;

&lt;/div&gt;

&lt;div&gt;
  
&lt;/div&gt;


          </content><id>http://blog.camlcity.org/blog/plasma4.html</id><title type="text">PlasmaFS</title><updated>2011-10-18T00:00:00-00:00</updated><author><name>camlcity</name></author></entry></feed>