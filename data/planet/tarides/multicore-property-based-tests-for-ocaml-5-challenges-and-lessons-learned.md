---
title: 'Multicore Property-Based Tests for OCaml 5: Challenges and Lessons Learned'
description: We summarise the challenges and lessons learned in developing a test
  suite of property-based tests to help ensure the correctness of OCaml 5.
url: https://tarides.com/blog/2024-12-23-multicore-property-based-tests-for-ocaml-5-challenges-and-lessons-learned
date: 2024-12-23T00:00:00-00:00
preview_image: https://tarides.com/blog/images/patrick-tomasso-1NTFSnV-KLs-unsplash-1360w.webp
authors:
- Tarides
source:
---

<p>In <a href="https://tarides.com/blog/2024-04-24-under-the-hood-developing-multicore-property-based-tests-for-ocaml-5/">a previous post</a>, we discussed how we have developed property-based tests (PBTs) to stress test the new runtime system in OCaml 5, and gave concrete examples of such tests. In this second part, we discuss some of the challenges and the lessons learned from that effort.</p>
<h2>Testing APIs With Hidden or Uncontrolled State</h2>
<p>In part 1, we saw how <code>STM</code> and <code>Lin</code> were useful to test stateful module interfaces, like <code>Float.Array</code>. The OCaml standard library and runtime however also expose modules that are stateful, but for which the state is hidden or outside the full control of a black-box testing process. We are nevertheless interested in helping ensure the correctness of such modules.</p>
<p>For example, <code>Ephemeron</code>s depend on the state of OCaml's heap, meaning that a garbage collection (generally out of control of the test driver) may unsuspectingly trigger and cause changes in the test outcome. As a result, we ended up abandoning <code>Lin</code> tests of <code>Ephemeron</code>s, as they would run multiple observations of the same system under test - one parallel observation and several sequential ones - most likely with different results, because of different garbage collection schedulings.</p>
<p>In addition, the <code>Lin</code> tests could cause excessive shrinking searches, in trying to find a minimal example causing <code>Ephemeron</code> differences between such runs. Instead we favoured replacing them with <code>STM</code> tests, that perform only <em>one observation</em> of the system under test. We furthermore experimented with <a href="https://github.com/ocaml-multicore/multicoretests/pull/367">inserting an explicit call to <code>Gc.full_major</code> in between our run of the sequential and parallel tests to increase the chance of starting the latter on a relatively stable heap basis</a>. We encountered <a href="https://github.com/ocaml-multicore/multicoretests/pull/365">similar problems with tests of the <code>Weak</code> module</a> and recently also in <a href="https://github.com/ocaml-multicore/multicoretests/pull/469">the <code>STM</code> tests of the <code>Gc</code> module</a>.</p>
<p>Despite these challenges, the above tests successfully found several issues, e.g.</p>
<ul>
<li><a href="https://github.com/ocaml/ocaml/pull/11749">racing <code>Weak</code> functions could in some cases produce strange values</a>,</li>
<li><a href="https://github.com/ocaml/ocaml/issues/11934">certain combinations of <code>Weak</code> <code>Hashset</code> functions could cause the runtime to <code>abort</code> or segfault</a>,</li>
<li><a href="https://github.com/ocaml/ocaml/issues/11503">the <code>Ephemeron</code> tests could trigger an assertion failure and abort</a>, and</li>
<li>out-of-date documentation for <a href="https://github.com/ocaml/ocaml/pull/13424"><code>Gc.quick_stat</code></a> and  <a href="https://github.com/ocaml/ocaml/pull/13440"><code>Gc.set</code></a></li>
</ul>
<h2>Cygwin Challenges</h2>
<p>As Cygwin support was being restored up in the 5.1 release, we wanted to test this platform as well to help ensure its correctness. However, we found that a test-suite run took an excessively long time, often not completing within a 6-hour timeout! We solved this initially by <a href="https://github.com/ocaml-multicore/multicoretests/pull/305">splitting a test-suite run into two separate workflows</a> and later by rephrasing it as <a href="https://github.com/ocaml-multicore/multicoretests/pull/313">two separate CI jobs, belonging to the same workflow</a>. Thanks to general improvements to the OCaml 5 runtime system, <a href="https://github.com/ocaml-multicore/multicoretests/pull/420">we have since been able to merge this split back into just one job</a> like the remaining platforms, and eventually also <a href="https://github.com/ocaml-multicore/multicoretests/pull/449">reduce the timeout for this single Cygwin workflow to 4 hours, like the remaining platforms</a>.</p>
<p>Another Cygwin challenge early on was <a href="https://cygwin.com/packages/summary/opam.html">the relatively old <code>opam.2.0.7</code> version it includes</a>. This made it harder to test the various OCaml compiler versions on Cygwin (and later MinGW). As a workaround, we set up <a href="https://github.com/shym/custom-opam-repository">a custom <code>opam</code> repository</a> with appropriate <code>opam</code> files for each compiler version.</p>
<h2>Keeping Dependencies Minimal</h2>
<p>Initially we happily used <code>ppxlib</code> to generate boilerplate code for QCheck generators using  <code>ppx_deriving_qcheck</code> and <code>show</code> functions for the tested <code>cmd</code>s. However, <code>ppxlib</code> depends on the OCaml compiler's AST which means that it occasionally breaks on the compiler's <code>trunk</code> development branch whenever its AST changes. As a consequence, testing <code>trunk</code> would halt until <code>ppxlib</code> was fixed again - an unfortunate situation when trying to help ensure its correctness! After <a href="https://github.com/ocaml-ppx/ppxlib/pull/407">helping keep a branch of <code>ppxlib</code> continuously working with <code>trunk</code></a>, at some point we instead decided to eliminate the test suite's <code>ppxlib</code> dependency. We therefore wrote the corresponding definitions by hand and by utilising a dedicated printing library <a href="https://ocaml-multicore.github.io/multicoretests/0.4/qcheck-multicoretests-util/Util/Pp/index.html"><code>Pp</code></a> in <a href="https://ocaml-multicore.github.io/multicoretests/0.4/qcheck-multicoretests-util/"><code>qcheck-multicoretests-util</code></a>. Since then, testing has not been blocked by such breakages, and the dependencies are down to just the <code>qcheck-core</code> package, and (transitively) <code>dune</code>.</p>
<p>Testing an increasing number of platforms such as the MinGW and Cygwin ports over the past 2-3 years has been challenging, as much of that effort predates <a href="https://opam.ocaml.org/blog/opam-2-2-0/">the more recent Windows support brought by opam-2.2</a>. As we additionally wanted to test OCaml 5's now restored MSVC port, also during its development, we ended up abandoning <code>opam</code> and <code>setup-ocaml</code> in favour of just building the tested compiler and our dependencies (<code>qcheck-core</code> and <code>dune</code>) from source in our CI workflows. As a result, we have gained a uniform CI workflow setup across platforms that also allows us to kick-off tests of feature branches, and thereby eliminate the need for the above-mentioned custom <code>opam</code> repository.</p>
<h2>Testing in the Presence of Misbehaviour</h2>
<p>Overall, when having found, investigated, reported, and sometimes also fixed an error, we would like to continue testing despite the existence of such known issues. A test-suite rerun is however likely to trigger and report the same bug again, temporarily hindering the discovery and fixing of other issues. Something similar has been observed by others, e.g. in <a href="https://www.erlang-factory.com/upload/presentations/582/CertifyingyourcarwithErlang.pdf">Quviq's property-based testing of AUTOSAR for Volvo</a>.</p>
<p>To be able to continue testing we can, in the simple cases, (temporarily) adjust the tested property to accept the observed (mis)behaviour. This was the case, for example, for <a href="https://github.com/ocaml/ocaml/pull/13424"><code>Gc.quick_stat</code> which would return non-zero entries for four record fields, where the documentation was out-of-date and specifying that zeros should be returned</a>.</p>
<p>However we have also had to (temporarily) adjust the generator to avoid triggering a particular buggy <code>cmd</code>. This was the case, for example, for <a href="https://github.com/ocaml/ocaml/pull/13370"><code>Gc.counters</code> which had an independently found-and-fixed issue with improper C interfacing with the GC</a>. <a href="https://github.com/ocaml-multicore/multicoretests/pull/469">Our new <code>Gc</code> test</a> would nevertheless trigger it and cause a crash on 5.2.0, until we adjusted the generator to skip generating <code>Gc.counters</code> calls on tests of versions up to 5.2.0. Since then <a href="https://discuss.ocaml.org/t/ocaml-5-2-1-released/15634">the fix has been included in the 5.2.1 bugfix release</a>.</p>
<p>Finally, we have also (temporarily) disabled a test on a platform. This is the case, for example, for <a href="https://github.com/ocaml/ocaml/issues/13046">the parallel test of <code>Dynlink</code> which is unsafe on Windows, due to an underlying flexdll issue</a>. Whereas <a href="https://github.com/ocaml/flexdll/pull/136">the flexdll issue is now fixed in the flexdll repository</a>, we are awaiting a new release before proceeding to re-enable the parallel test on Windows.</p>
<h2>Crashes Take Down the Test Runner Too</h2>
<p>Since the test-driver is running in OCaml too – and in the same process – when the SUT crashes, so does the entire QCheck test-driver process. In our experience this happens more often than not, as runtime issues tend to lead to memory corruption and typically a <a href="https://en.wikipedia.org/wiki/Segmentation_fault">segmentation fault</a>. Running the test in a <a href="https://en.wikipedia.org/wiki/Fork_(system_call)"><code>fork</code>ed</a> child process may guard against a crash in the child taking down the parent process, with the caveat that OCaml 5 prevents <code>fork</code>ing child processes after the first <code>Domain.spawn</code>. In the spirit of functional programming, this option is available as a reusable combinator <a href="https://ocaml-multicore.github.io/multicoretests/0.4/qcheck-multicoretests-util/Util/index.html#val-fork_prop_with_timeout"><code>Util.fork_prop_with_timeout : int -&gt; ('a -&gt; bool) -&gt; 'a -&gt; bool</code></a> in <a href="https://ocaml.org/p/qcheck-multicoretests-util/latest"><code>qcheck-multicoretests-util</code></a>, thus allowing us to easily wrap the property of a crash-triggering <code>QCheck</code> test.</p>
<p>Despite not designed with any of the above in mind, we have arrived at a test layout where most tests are run as separate executables, which lets us identify crashes relatively easily and simultaneously lets a test-suite run continue despite encountering a crash underway.</p>
<h2>Positive Testing, Negative Testing, and Stress Testing</h2>
<p>While developing the <code>STM</code> and <code>Lin</code> libraries it became clear that we should guard against changes mistakenly affecting their error-finding behaviour. For this reason, we added negative tests of e.g. parallel modifications of an unprotected <code>ref</code> cell that are expected to fail, and then <a href="https://github.com/c-cube/qcheck/pull/244">extended <code>QCheck</code> with a <code>Test.make_neg</code> function to construct a negative test</a>. In the CI we then use this negative testing ability e.g. to test that <code>Hashtbl</code> is unsafe to use in parallel as mentioned in part 1 and that the generator can find a counterexample illustrating it.</p>
<p>Often, such a parallel negative test triggers quickly, e.g. on one of the first 10 test inputs generated – effectively stress-testing a module under parallel usage for only a very limited amount of time. We therefore added stress test properties, in the form of <code>stress_test</code> for <code>Lin</code> and <code>stress_test_par</code> for <code>STM</code>, both offering a weaker, more forgiving property that only fails on unexpected exceptions or outright crashes, thus strengthening our belief in the runtime - even under longer, continued parallel misusage.</p>
<p>Effectively, we have arrived at PBT variants of classical test concepts:</p>
<ul>
<li><strong>positive tests</strong> - expected to hold across many random test inputs</li>
<li><strong>negative tests</strong> - expected not to hold and produce a counterexample</li>
<li><strong>stress tests</strong> - expected not to misbehave by raising an exception or crashing</li>
</ul>
<h2>False Alarms</h2>
<p>As more and more CI target platforms have been added, we have also seen a variation in behaviour across them: Some of the above negative tests are not triggered as consistently across platforms and some of the tests take a long time or cause timeouts on some platforms.</p>
<p>These add noise and still require us to check whether a failure was genuine or not. We have therefore focused on reducing the noise from false positives. To better understand this effort here is a plot of CI workflow outcomes for merged PRs (361-486) spanning a period of ~1.5 years from early June 2023 to early December 2024:</p>
<p><img src="https://tarides.com/blog/images/false-alarms-plot-1360w~dX4NQFJuZHoHK0QiRAid_g.webp" sizes="(min-width: 1360px) 1360px, (min-width: 680px) 680px, 100vw" srcset="/blog/images/false-alarms-plot-170w~kfpgpUAdT9uV0ZkyJt555g.webp 170w, /blog/images/false-alarms-plot-340w~XMa35cJ4nLEu0s52QbwbBQ.webp 340w, /blog/images/false-alarms-plot-680w~eQrc8jI6bcA5Lhq0n8SNLw.webp 680w, /blog/images/false-alarms-plot-1360w~dX4NQFJuZHoHK0QiRAid_g.webp 1360w" alt="A stacked histogram illustrating the outcome of CI workflow runs split into 'OK', 'ci', 'genuine', and 'other' categories"></p>
<p>First of all, this covers a period of testing a mix of OCaml versions, starting with workflows targeting 5.1 in June 2023, then 5.2, and now 5.3 and 5.4/trunk (the current compiler development version). Note that this only plots the outcome for Jan's PRs to <code>main</code> for a fair comparison, as these would originally trigger twice as many workflow runs (push and PR). It furthermore only includes the last run for each PR, in case there were more because of PR revisions. Each CI run may in principle trigger several errors (in different categories even!). This is a rare incident however.</p>
<p>Further notes:</p>
<ul>
<li>On 370 a ppc64 workflow was added (workflow number increase)</li>
<li>391 added framepointer workflows (workflow number increase)</li>
<li>392-393 revealed <a href="https://github.com/ocaml/ocaml/issues/12543">a cmi-file lookup regression</a> that made several workflows fail, hence the spike</li>
<li>There were CI and network issue around 395-398</li>
<li>On 396 a first FreeBSD workflow was added (workflow number increase)</li>
<li>On 398 a second FreeBSD workflow and extra opam install workflows were added (workflow number increase)</li>
<li>On 420 the 2-split Cygwin workflows were merged (workflow number decrease)</li>
<li>On 429 (merged before 431) we eliminated duplicate CI runs for both push and the PR</li>
<li>On 438 we retired the 5.1 workflows to only run weekly (not on every PR)</li>
<li>On 449 the older MSVC PR (399) adding 2 additional MSVC workflows had just been merged</li>
<li>On 453 the parallel <code>Dynlink</code> tests were disabled on Windows (since a fix had been developed and offered on the FlexDLL repo)</li>
<li>On 458 2 macOS ARM64 workflows were temporarily duplicated while moving them from <code>multicoretests-ci</code> to GitHub actions</li>
<li>On 471 <code>5.4.0+trunk</code> workflows were added and removed three 5.1 multicoretests-ci workflows</li>
<li>On 481 <code>multicoretests-ci</code> stopped running the 4 remaining 5.2 workflows</li>
<li>On 482 the ten 5.2 workflows were disabled</li>
</ul>
<p>The smaller sub-bars are harder to distinguish with the dominant OK bars in the above figure.  Below we therefore zoom in and display the same plot, including only the different kinds of failures:</p>
<p><img src="https://tarides.com/blog/images/false-alarms-plot-errors-only-1360w~wOpCubYg66VDTbHXaZMcZw.webp" sizes="(min-width: 1360px) 1360px, (min-width: 680px) 680px, 100vw" srcset="/blog/images/false-alarms-plot-errors-only-170w~5YZPBXgUoTrAIc0KQle6iw.webp 170w, /blog/images/false-alarms-plot-errors-only-340w~5HTdqWao21Ru8BWhPQSXJA.webp 340w, /blog/images/false-alarms-plot-errors-only-680w~ulyPw_CnsHR2OYtym2eM_A.webp 680w, /blog/images/false-alarms-plot-errors-only-1360w~wOpCubYg66VDTbHXaZMcZw.webp 1360w" alt="A stacked histogram illustrating the outcome of CI workflow runs split, focusing only on the 'ci', 'genuine', and 'other' error categories"></p>
<p>There are multiple competing efforts and aspects behind the amount of 'genuine' errors triggered in the above:</p>
<ul>
<li>First, OCaml developers have fixed a number of defects and released 5.1.0, 5.2.0, 5.2.1, and 5.3.0~beta2 over this time period</li>
<li>Second, as new compiler features are added and merged they may accidentally introduce new errors</li>
<li>Third, we have added tests and 'sharpened the axe' of the existing ones</li>
<li>Finally, a fix of a bug in 5.1 merged into 5.2 doesn't prevent the bug from continuing to show up on 5.1 CI runs</li>
</ul>
<p>It is clear from the plot that both genuine issues (categorised as 'genuine') and false alarms (categorised as 'other') have decreased over this period. The workflow alarms were initially dominated by false ones, then started being dominated by genuine (and 'ci') ones, and have now settled on a level with zero alarms being a common outcome. Such a noise-free test-suite signal is also central for OCaml compiler developers to utilise the test suite in their own development workflow. Here a test-suite red light should ideally signal a problem with a proposed runtime change, rather than (a) false alarms adding needless noise ("oh, never mind that red light!") and (b) true alarms adding genuine noise ("actually that's an existing unfixed issue, not your fault").</p>
<p>To understand the failures triggering on specific versions, here's first a plot of the outcomes for the weekly CI runs of the upcoming 5.3 release:</p>
<p align="center">
  <img src="https://tarides.com/blog/images/workflow-ci-runs-53-1360w~587YI9GedCt9uz4sgqe0YQ.webp" sizes="(min-width: 1360px) 1360px, (min-width: 680px) 680px, 100vw" srcset="/blog/images/workflow-ci-runs-53-170w~MAxMQ7TiFvt_peAArj2ViA.webp 170w, /blog/images/workflow-ci-runs-53-340w~IZK0D8uJo6yhFuDNGNy3NA.webp 340w, /blog/images/workflow-ci-runs-53-680w~6MVyB_rf2AQZJ_1ML-jJCw.webp 680w, /blog/images/workflow-ci-runs-53-1360w~587YI9GedCt9uz4sgqe0YQ.webp 1360w" alt="A stacked histogram illustrating the outcome of 5.3 CI workflow runs split into 'OK' and 'Fail' categories across 3 months, showing only one failure" width="45%">
</p>
<p>The one failure on October 21 happened during Cygwin installation, and was hence not related to the test suite. In comparison, weekly runs of the 5.0 workflows tend to trigger issues. This below plot covers 8 workflows in contrast to 5.3's 12 workflows, as Cygwin, frame-pointer, and MSVC byte/native workflows were added since. Since the 5.0 release is older than 5.3, we also started recording weekly workflow outcomes for it sooner:</p>
<p align="center">
  <img src="https://tarides.com/blog/images/workflow-ci-runs-50-1360w~TBemZzEzBagiSN81hfbXbQ.webp" sizes="(min-width: 1360px) 1360px, (min-width: 680px) 680px, 100vw" srcset="/blog/images/workflow-ci-runs-50-170w~oh9VdUd6FLX7B-YdoJ58_w.webp 170w, /blog/images/workflow-ci-runs-50-340w~glF_vLQa_We6QdlOasRkAg.webp 340w, /blog/images/workflow-ci-runs-50-680w~iJAugUPo8KRE56egXeVXsQ.webp 680w, /blog/images/workflow-ci-runs-50-1360w~TBemZzEzBagiSN81hfbXbQ.webp 1360w" alt="A stacked histogram illustrating the outcome of 5.0 CI workflow runs split into 'OK' and 'Fail' categories across 3 months, showing failures in all but 4 out of 22 runs" width="45%">
</p>
<p>The plot clearly indicates that issues are triggered on 5.0 workflows more often that not. Ever since starting to note the cause of such failures in late August 2024, the triggered issues are genuine, and typically include <a href="https://github.com/ocaml/ocaml/issues/12103">crashes due to parallel access to <code>Buffer</code></a>, pinpointing a case in <code>Buffer.add_string</code> that flew under the radar of <a href="https://github.com/ocaml/ocaml/pull/11742">the first <code>Buffer</code> fix included in the 5.0.0 release</a>.</p>
<h2>Hidden Costs</h2>
<p>We have worked to reach zero false alarms and now generally achieve it across an array of 31 CI workflows. We apply due diligence and have thus developed a workflow of going over the CI red lights to understand and summarise the failures – both the genuine ones and the false alarms. We then keep track of them as <a href="https://github.com/ocaml-multicore/multicoretests/issues">issues in the multicoretests repo</a>. This allows us to easily refer to them and spot trends, such as starting to see new failures or noticing that a particular failure no longer occurs.</p>
<p>When we spot new errors, we work to reproduce them locally to make sure the issue is genuine. If so, we then report the issue upstream to <code>ocaml/ocaml</code> along with a description of the required steps needed to reproduce it. When understanding the problem well enough, we also contribute with a compiler fix PR. Out of <a href="https://github.com/ocaml-multicore/multicoretests#issues">the 40 issues currently identified</a>, Tarides engineers have filed PRs to fix 28 of these, 10 issues have been fixed by others (typically Inria or Jane Street engineers), and 2 issues remain open. Tarides have thus put in a significant effort to resolve errors.</p>
<h2>Some Issues are Still Hard to Reproduce</h2>
<p>Despite all our efforts to amplify problems and increase reproducibility, some issues are still hard to trigger. One such case was <a href="https://github.com/ocaml/ocaml/pull/12707">ocaml/ocaml#12707</a> in which we were able to trigger the assertion failure, albeit rarely. This one took some head-scratching until we realised the problem was caused by a small time window between reading the same atomic field twice in an assertion: <code>di-&gt;backup_thread_msg == BT_INIT || di-&gt;backup_thread_msg == BT_TERMINATE</code>. This was carried out in parallel with a backup-thread transitioning from <code>BT_TERMINATE</code> to <code>BT_INIT</code> by an atomic write <code>atomic_store_release(&amp;di-&gt;backup_thread_msg, BT_INIT)</code>, thus creating a tiny chance of neither of the conditions to be true, if the write would happen just in between the two reads. We could then manually insert a call to <code>sleep</code> to confirm, and develop an appropriate fix.</p>
<p>We are currently investigating <a href="https://github.com/ocaml-multicore/multicoretests/issues/480">an even rarer issue triggered by the <code>Gc</code> tests, that causes a rare crash on macOS running on an ARM64 processor</a> and seems to require just the right OCaml heap conditions to trigger. In both cases, despite not triggering on every PBT run, the randomised tests have nevertheless highlighted genuine issues that would otherwise only show up even more rarely on the ocaml/ocaml test suite or – worse – to end users of OCaml.</p>
<h2>Lowering the Barrier to Entry for <code>multicoretests</code> for Compiler Engineers</h2>
<p>To offer compiler engineers the ability to run the test suite easily, <a href="https://github.com/ocaml/ocaml/pull/13458">we have made it possible to do so by labeling a PR with a <code>run-multicoretests</code> tag</a>. Recently <a href="https://github.com/ocaml/ocaml/pull/13580">a PR to improve major GC performance with mark-delay</a> kicked off using the <code>run-multicoretests</code> tag and is already making good use of the test suite as <a href="https://github.com/ocaml/ocaml/pull/13580#issuecomment-2478454501">it detected an issue with the GC marking of Ephemerons</a>. With <a href="https://github.com/ocaml/ocaml/pull/13616">the test suite finding an issue in another GC improving PR</a>, we are confident in the value addition that the test suite brings to compiler developer in helping quality-assure runtime-related PRs.</p>
<h2>Usage Outside <code>multicoretests</code></h2>
<p>The usage of <code>STM</code> has spread outside of <code>multicoretests</code>. In <a href="https://github.com/ocaml-multicore/saturn">Saturn</a>, a library of lock-free data structures for OCaml 5, both existing and new data structures come with <code>STM</code> tests to help ensure their correctness. This started in connection with <a href="https://github.com/ocaml-multicore/saturn/pull/43">moving experimental tests of <code>ws_deque</code> out of the <code>multicoretests</code> suite</a> and has continued since. In <a href="https://github.com/ocaml-multicore/picos">Picos</a>, a library for composing effect-based schedulers, <code>STM</code> tests also help ensure correctness of its underlying data structures.</p>
<p>For the Gospel specification language for OCaml, Tarides has worked to develop <a href="https://github.com/ocaml-gospel/ortac">Ortac-QCheck-STM, as a plugin for Ortac</a>. This is a tool to extract sequential <code>STM</code> tests from a Gospel specification, thereby putting the strength of PBT in the hands of OCaml developers willing to annotate their interfaces with Gospel specifications. This effort is paying off, as <a href="https://github.com/ocaml-gospel/ortac?tab=readme-ov-file#found-issues">the tool is starting to find genuine issues</a>.</p>
<h2>Conclusion</h2>
<p>Using PBT to test OCaml 5 started as a blue-sky project at Tarides. Despite a range of challenges, the test suite has nevertheless reached a point where a run yields a clear signal free of false alarms and worthy of a confidence increase in a compiler code change. Getting here was a team effort with contributions from Charlène Gros, Samuel Hym, Olivier Nicole, Nicolas Osborne, and Naomi Spargo, along with patience and effort from OCaml compiler engineers working to fix our reported findings.</p>
<p>The PBT approach has successfully pinpointed a range of issues across the <code>Stdlib</code>, the rewritten OCaml 5 runtime system, and the restored backends. We hope that the test suite can continue to do so and thereby help maintain OCaml's reputation as a rock solid and safe platform.</p>

